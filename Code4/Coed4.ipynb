{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/zhpinkman/armed-bandit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ./armed-bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impor Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mBMoPLmGbrIn"
   },
   "outputs": [],
   "source": [
    "from amalearn.reward import RewardBase\n",
    "from amalearn.agent import AgentBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YBACGmh0brIr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from amalearn.environment import EnvironmentBase\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "id": "pH6sNHxPbrIs"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Environment(EnvironmentBase):\n",
    "    def __init__(self, obstacle = [] ,id = 0, action_count=9, actionPrice = -1, stopActionPrice = -1, goalReward = 100\n",
    "                 , punish=-10, j_limit = 10, i_limit = 10, p = 0.8, container=None):\n",
    "        \"\"\"\n",
    "        initialize your variables\n",
    "        \"\"\"\n",
    "        \n",
    "        self.obstacle = obstacle\n",
    "        \n",
    "        self.x_min = 1\n",
    "        self.x_max = i_limit\n",
    "        \n",
    "        self.y_min = 1\n",
    "        self.y_max = j_limit\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "        self.action_count = action_count\n",
    "        self.actionPrice = actionPrice\n",
    "        self.goalReward = goalReward\n",
    "        self.punish = punish\n",
    "        self.p = p\n",
    "        \n",
    "        action_space = Discrete(action_count)\n",
    "        state_space = Box(low=1, high=max(i_limit, j_limit), shape=(1,2), dtype=int)\n",
    "        \n",
    "        ######################\n",
    "        ###     Action     ###\n",
    "        ###                ###\n",
    "        ### 0 1 2  ↖ ↑ ↗   ###\n",
    "        ### 3 4 5  ← • →   ###\n",
    "        ### 6 7 8  ↙ ↓ ↘  ###\n",
    "        #####################\n",
    "        \n",
    "        #####################\n",
    "        ###     State     ###\n",
    "        ###               ###\n",
    "        ###    (i, j)     ###\n",
    "        #####################\n",
    "        \n",
    "        self.action_list = list(range(1,10))\n",
    "        self.state_list = []\n",
    "        \n",
    "        for i in range(1, i_limit+1):\n",
    "            for j in range(1, j_limit+1):\n",
    "                self.state_list.append(np.array([i, j]))\n",
    "        \n",
    "        super(Environment, self).__init__(action_space=action_space, state_space=state_space, id=id ,container=container)\n",
    "\n",
    "        \n",
    "    def isStatePossible(self, state):\n",
    "        \"\"\"if given state is possible (not out of the grid and not obstacle) return ture\"\"\"\n",
    "        if self.x_min <= state[0] <= self.x_max and self.y_min <= state[1] <= self.y_max:\n",
    "            for obstacle_item in self.obstacle:\n",
    "                if (state==obstacle_item).all():\n",
    "                    return False\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def isAccessible(self, state, state_p):\n",
    "        \"\"\"if given state is Accesible (we can reach state_p by doing an action from state) return true\"\"\"\n",
    "        return abs(state[0]-state_p[0]) <= 1 and abs(state[1] - state_p[1]) <= 1 and self.isStatePossible(state_p)\n",
    "            \n",
    "    def getTransitionStatesAndProbs(self, state, action, state_p):\n",
    "        \"\"\"return probability of transition or T(sp,a,s)\"\"\"\n",
    "        \n",
    "        actions = self.available_actions_state(state)\n",
    "        \n",
    "        if (self.calculate_next_state(state,action)==state_p).all():\n",
    "            if self.isAccessible(state, state_p):\n",
    "                return self.p + (1-self.p) / len(actions)\n",
    "            else:\n",
    "                return self.p\n",
    "        else:\n",
    "            if self.isAccessible(state, state_p):\n",
    "                return (1-self.p) / len(actions)\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "    def getReward(self, state, action, state_p):\n",
    "        \"\"\"return reward of transition\"\"\"\n",
    "        \n",
    "        if action == 4:\n",
    "            action_price = self.stopActionPrice\n",
    "        else:\n",
    "            action_price = self.actionPrice\n",
    "        \n",
    "        if self.terminated_state(state_p):\n",
    "            return action_price + self.goalReward\n",
    "        \n",
    "        if self.isStatePossible(state_p):\n",
    "            return action_price\n",
    "        else:\n",
    "            return action_price + self.punish\n",
    "        \n",
    "    def sample_all_rewards(self):\n",
    "        return \n",
    "    \n",
    "    def calculate_reward(self, action):\n",
    "        return self.getReward(self.current_state, action, self.calculate_next_state(self.current_state, action))\n",
    "\n",
    "    def available_states_state(self, state):\n",
    "        states = []\n",
    "        for i in [-1, 0, +1]:\n",
    "            for j in [-1, 0, +1]:\n",
    "                new_state = np.array([state[0]+i, state[1]+j])\n",
    "                \n",
    "                if self.isAccessible(state, new_state):\n",
    "                    states.append(new_state)\n",
    "        return states\n",
    "    \n",
    "    def terminated(self):\n",
    "        return self.terminated_state(self.current_state)\n",
    "    \n",
    "    def terminated_state(self, state):\n",
    "        return (state==np.array([1,1])).all()\n",
    "        \n",
    "    def observe(self):\n",
    "        return self.current_state \n",
    "\n",
    "    def available_actions(self):\n",
    "        return self.available_actions_state(self.current_state)\n",
    "    \n",
    "    def available_actions_state(self, state):\n",
    "        output_actions = []\n",
    "        for action in range(self.action_count):\n",
    "            next_state = self.calculate_next_state(state, action)\n",
    "            \n",
    "            if self.isAccessible(state, next_state):\n",
    "                output_actions.append(action)\n",
    "        \n",
    "        return output_actions\n",
    "        \n",
    "    \n",
    "    def calculate_next_state(self, state, action):\n",
    "        return np.array([state[0] + (action%3 -1), state[1] + (int(action/3)-1) ])\n",
    "        \n",
    "    def next_state(self, action):\n",
    "        actions = self.available_actions()\n",
    "        \n",
    "        if action not in actions:\n",
    "            actions.append(action)\n",
    "                \n",
    "        probabilities = []\n",
    "                \n",
    "        for action2 in actions:\n",
    "            state2 = self.calculate_next_state(self.current_state, action2)\n",
    "            probabilities.append(self.getTransitionStatesAndProbs(self.current_state, action, state2))\n",
    "        \n",
    "        final_action = random.choices(population=actions, weights=probabilities, k=1)[0]\n",
    "        \n",
    "        real_next_state = self.calculate_next_state(self.current_state, final_action)\n",
    "        \n",
    "        if not self.isStatePossible(real_next_state):\n",
    "            real_next_state = self.current_state\n",
    "        \n",
    "        self.last_action = action\n",
    "        \n",
    "        self.sliped = not (final_action==action)\n",
    "        \n",
    "        self.current_state = real_next_state\n",
    "        \n",
    "        return\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = np.array([15, 15])\n",
    "        \n",
    "        self.last_action = None\n",
    "        self.sliped = None\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"{self.current_state} \\t {self.last_action} \\t {self.sliped}\")\n",
    "        return \n",
    "\n",
    "    def close(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_states =  [np.array([7, 1]), np.array([8, 1]), np.array([7, 2]), np.array([8, 2])\n",
    "                ,np.array([7, 3]), np.array([8, 3]), np.array([7, 4]), np.array([8, 4])\n",
    "                ,np.array([13, 8]), np.array([14, 8]), np.array([15, 8])\n",
    "                ,np.array([13, 9]), np.array([14, 9]), np.array([15, 9])\n",
    "                ,np.array([6, 12]), np.array([7, 12]), np.array([6, 13]), np.array([7, 13])\n",
    "                ,np.array([6, 14]), np.array([7, 14]), np.array([6, 15]), np.array([7, 15])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "id": "898Jlhsycyes"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Agent(AgentBase):\n",
    "    def __init__(self, id, environment, discount, theta):\n",
    "        self.environment = environment\n",
    "        \n",
    "        # init V\n",
    "        self.V = {}\n",
    "        \n",
    "        # init policy\n",
    "        self.policy = {}\n",
    "        \n",
    "        super(Agent, self).__init__(id, environment)\n",
    "        \n",
    "        self.discount = discount\n",
    "        \n",
    "        self.theta = theta\n",
    "\n",
    "        # initialize a random policy and V(s) = 0 for each state        \n",
    "        self.value_initialization()\n",
    "        \n",
    "        self.policy_initialization()\n",
    "    \n",
    "    def value_initialization(self):\n",
    "        for state in self.environment.state_list:\n",
    "            self.V[tuple(state)] = 0\n",
    "        \n",
    "    def policy_initialization(self):\n",
    "        for state in self.environment.state_list:\n",
    "            self.policy[tuple(state)] = random.choice(self.environment.action_list)\n",
    "        \n",
    "    def policy_evaluation(self):\n",
    "        new_V = {}\n",
    "\n",
    "        delta = 0\n",
    "        for state in self.environment.state_list:\n",
    "            \n",
    "            available_states  = self.environment.available_states_state(state)\n",
    "\n",
    "            action = self.policy[tuple(state)]\n",
    "            \n",
    "            sum = 0\n",
    "            \n",
    "            for state_p in available_states:\n",
    "                p_sp = self.environment.getTransitionStatesAndProbs(state, action, state_p)\n",
    "                r_sp = self.environment.getReward(state, action, state_p)\n",
    "                v_sp = self.V[tuple(state_p)]\n",
    "\n",
    "                sum += p_sp * (r_sp + self.discount * v_sp)\n",
    "\n",
    "            new_V[tuple(state)] = sum\n",
    "            delta = max(delta, abs(self.V[tuple(state)] - new_V[tuple(state)]))\n",
    "        \n",
    "        print(f\"iter = {iter} -> delta = {round(delta, 2)}\")\n",
    "        self.V = deepcopy(new_V)\n",
    "\n",
    "        if delta < self.theta[\"delta_treshold\"]:\n",
    "            print(\"Value convergenced\")\n",
    "\n",
    "        return delta < self.theta[\"delta_treshold\"]\n",
    "    \n",
    "    def policy_improvement(self):\n",
    "        policy_stable = True\n",
    "        \n",
    "        new_policy = {}\n",
    "        \n",
    "        for state in self.environment.state_list:\n",
    "\n",
    "            available_actions = self.environment.available_actions_state(state)\n",
    "            available_states  = self.environment.available_states_state(state)\n",
    "            \n",
    "            max_value = -math.inf\n",
    "            argmax = None\n",
    "            \n",
    "            for action in available_actions:\n",
    "                sum = 0\n",
    "                for state_p in available_states:\n",
    "                    p_sp = self.environment.getTransitionStatesAndProbs(state, action, state_p)\n",
    "                    v_sp = self.V[tuple(state_p)]\n",
    "\n",
    "                    sum += p_sp * v_sp\n",
    "            \n",
    "                if sum > max_value:\n",
    "                    max_value = sum\n",
    "                    argmax = action\n",
    "                    \n",
    "            new_policy[tuple(state)] = argmax\n",
    "            \n",
    "            if self.policy[tuple(state)] != argmax:\n",
    "                policy_stable = False\n",
    "        \n",
    "        self.policy = deepcopy(new_policy)\n",
    "        if policy_stable:\n",
    "            print(\"Policy convergenced\")\n",
    "            \n",
    "        return policy_stable\n",
    "    \n",
    "    \n",
    "    def policy_iteration(self):\n",
    "        flag_value = self.policiy_evaluation()\n",
    "        \n",
    "        flag_policy = self.policy_improvement()\n",
    "        \n",
    "        if flag_value or flag_policy:\n",
    "            return\n",
    "    \n",
    "    def value_iteration(self):\n",
    "        for iter in range(self.theta[\"max_iter\"]):\n",
    "            new_V = {}\n",
    "\n",
    "            delta = 0\n",
    "            for state in self.environment.state_list:\n",
    "                new_V[tuple(state)] = -math.inf\n",
    "\n",
    "                available_actions = self.environment.available_actions_state(state)\n",
    "                available_states  = self.environment.available_states_state(state)\n",
    "\n",
    "                for action in available_actions:\n",
    "                    sum = 0\n",
    "                    for state_p in available_states:\n",
    "                        p_sp = self.environment.getTransitionStatesAndProbs(state, action, state_p)\n",
    "                        r_sp = self.environment.getReward(state, action, state_p)\n",
    "                        v_sp = self.V[tuple(state_p)]\n",
    "\n",
    "                        sum += p_sp * (r_sp + self.discount * v_sp)\n",
    "\n",
    "                    new_V[tuple(state)] = max(new_V[tuple(state)], sum)\n",
    "                delta = max(delta, abs(self.V[tuple(state)] - new_V[tuple(state)]))\n",
    "                \n",
    "            print(f\"iter = {iter} -> delta = {round(delta, 2)}\")\n",
    "            self.V = deepcopy(new_V)\n",
    "\n",
    "            if delta < self.theta[\"delta_treshold\"]:\n",
    "                print(\"Value convergenced\")\n",
    "                break\n",
    "        \n",
    "        self.policy_extraction()\n",
    "    \n",
    "    def policy_extraction(self):\n",
    "        for state in self.environment.state_list:\n",
    "\n",
    "            available_actions = self.environment.available_actions_state(state)\n",
    "            available_states  = self.environment.available_states_state(state)\n",
    "            \n",
    "            max_value = -math.inf\n",
    "            argmax = None\n",
    "            \n",
    "            for action in available_actions:\n",
    "                sum = 0\n",
    "                for state_p in available_states:\n",
    "                    p_sp = self.environment.getTransitionStatesAndProbs(state, action, state_p)\n",
    "                    v_sp = self.V[tuple(state_p)]\n",
    "\n",
    "                    sum += p_sp * v_sp\n",
    "            \n",
    "                if sum > max_value:\n",
    "                    max_value = sum\n",
    "                    argmax = action\n",
    "                    \n",
    "            self.policy[tuple(state)] = argmax\n",
    "    \n",
    "    def print_value(self):\n",
    "        p = PrettyTable()\n",
    "\n",
    "        for j in range(1,16):\n",
    "            row = []\n",
    "            for i in range(1, 16):\n",
    "                if self.environment.isStatePossible(np.array([i,j])):\n",
    "                    row.append(int(self.V[(i,j)]))\n",
    "                else:\n",
    "                    row.append(\"###\")\n",
    "                \n",
    "            p.add_row(row)\n",
    "\n",
    "        print (p.get_string(header=False, border=True))\n",
    "    \n",
    "    def print_policy(self):\n",
    "        p = PrettyTable()\n",
    "\n",
    "        for j in range(1,16):\n",
    "            row = []\n",
    "            for i in range(1, 16):\n",
    "                if self.environment.isStatePossible(np.array([i,j])):\n",
    "                    row.append(self.action_symbol(int(self.policy[(i,j)])))\n",
    "                else:\n",
    "                    row.append(\"▮\")\n",
    "                \n",
    "                \n",
    "            p.add_row(row)\n",
    "\n",
    "        print (p.get_string(header=False, border=True))\n",
    "    \n",
    "    def take_action(self) -> (object, float, bool, object):\n",
    "        # observation, reward, done, info\n",
    "        return self.environment.step(random.choice(self.environment.action_list))\n",
    "    \n",
    "    def action_symbol(self, action):\n",
    "        if action == 0:\n",
    "            return \"↖\"\n",
    "        elif action == 1:\n",
    "            return \"↑\"\n",
    "        elif action == 2:\n",
    "            return \"↗\"\n",
    "        elif action == 3:\n",
    "            return \"←\"\n",
    "        elif action == 4:\n",
    "            return \"•\"\n",
    "        elif action == 5:\n",
    "            return \"→\"\n",
    "        elif action == 6:\n",
    "            return \"↙\"\n",
    "        elif action == 7:\n",
    "            return \"↓\"\n",
    "        elif action == 8:\n",
    "            return \"↘\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = -0.01, stopActionPrice = -0.01\n",
    "                               ,goalReward = 1000, punish=-1, j_limit = 15, i_limit = 15, p = 0.8, container=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = {\"max_iter\": 5, \"delta_treshold\": 3}\n",
    "agent = Agent(id=0, environment=base_environment, discount=0.9, theta=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0 -> delta = 84.0\n",
      "iter = 1 -> delta = 75.33\n",
      "iter = 2 -> delta = 66.97\n",
      "iter = 3 -> delta = 60.19\n",
      "iter = 4 -> delta = 54.09\n"
     ]
    }
   ],
   "source": [
    "agent.value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+----+-----+-----+-----+----+----+----+----+-----+-----+-----+\n",
      "| 340 | 332 | 232 | 147 | 76 |  27 | ### | ### | -4 | -4 | -4 | -4 |  -4 |  -4 |  -4 |\n",
      "| 332 | 327 | 230 | 146 | 76 |  27 | ### | ### | -4 | -4 | -4 | -4 |  -4 |  -4 |  -4 |\n",
      "| 232 | 230 | 221 | 143 | 75 |  27 | ### | ### | -4 | -4 | -4 | -4 |  -4 |  -4 |  -4 |\n",
      "| 147 | 146 | 143 | 132 | 72 |  26 | ### | ### | -4 | -4 | -4 | -4 |  -4 |  -4 |  -4 |\n",
      "|  76 |  76 |  75 |  72 | 62 |  23 |  -4 |  -4 | -4 | -4 | -4 | -4 |  -4 |  -4 |  -4 |\n",
      "|  26 |  26 |  26 |  26 | 23 |  20 |  -4 |  -4 | -4 | -4 | -4 | -4 |  -4 |  -4 |  -4 |\n",
      "|  -4 |  -4 |  -4 |  -4 | -4 |  -4 |  -4 |  -4 | -4 | -4 | -4 | -4 |  -4 |  -4 |  -4 |\n",
      "|  -4 |  -4 |  -4 |  -4 | -4 |  -4 |  -4 |  -4 | -4 | -4 | -4 | -4 | ### | ### | ### |\n",
      "|  -4 |  -4 |  -4 |  -4 | -4 |  -4 |  -4 |  -4 | -4 | -4 | -4 | -4 | ### | ### | ### |\n",
      "|  -4 |  -4 |  -4 |  -4 | -4 |  -4 |  -4 |  -4 | -4 | -4 | -4 | -4 |  -4 |  -4 |  -4 |\n",
      "|  -4 |  -4 |  -4 |  -4 | -4 |  -4 |  -4 |  -4 | -4 | -4 | -4 | -4 |  -4 |  -4 |  -4 |\n",
      "|  -4 |  -4 |  -4 |  -4 | -4 | ### | ### |  -4 | -4 | -4 | -4 | -4 |  -4 |  -4 |  -4 |\n",
      "|  -4 |  -4 |  -4 |  -4 | -4 | ### | ### |  -4 | -4 | -4 | -4 | -4 |  -4 |  -4 |  -4 |\n",
      "|  -4 |  -4 |  -4 |  -4 | -4 | ### | ### |  -4 | -4 | -4 | -4 | -4 |  -4 |  -4 |  -4 |\n",
      "|  -4 |  -4 |  -4 |  -4 | -4 | ### | ### |  -4 | -4 | -4 | -4 | -4 |  -4 |  -4 |  -4 |\n",
      "+-----+-----+-----+-----+----+-----+-----+-----+----+----+----+----+-----+-----+-----+\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal Value\")\n",
    "agent.print_value()\n",
    "\n",
    "print(\"\\nOptimal Policy\")\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "| • | ← | ← | ← | ← | ← | ▮ | ▮ | • | ↓ | ↙ | ← | ↘ | ↓ | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ← | ↖ | ↖ | ↖ | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↗ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↗ | ← | ↖ | ↖ | ↘ | ↖ | ↖ | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↘ | ↓ | ↖ | ← | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↓ | ↘ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↗ | ↗ | ↗ | ↗ | ↗ | ↗ | ↗ | ↑ | ↘ | ↑ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↗ | ↑ | ↖ | ↖ | ↖ | ↗ | ↑ | ↖ | ↘ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↗ | ↘ | ↘ | ↘ | ↘ | ← | ↗ | ↑ | ↗ | ↖ | ↖ | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↘ | ↖ | ↖ | ↖ | ← | ← | ↗ | ↗ | ↑ | ↖ | ↖ | ↖ |\n",
      "| ↓ | ↘ | ← | ← | ↖ | ▮ | ▮ | ↖ | ↖ | ← | ↘ | ↙ | ← | ↖ | ↖ |\n",
      "| • | ← | ← | ← | ↙ | ▮ | ▮ | ↘ | ↘ | ↘ | ← | ← | ← | ← | ↙ |\n",
      "| ↑ | ↖ | ↖ | ← | ← | ▮ | ▮ | ↑ | ↖ | ← | ← | ← | ↖ | ← | ← |\n",
      "| ↑ | ↖ | ↑ | ↖ | ↖ | ▮ | ▮ | ↗ | ↑ | ↖ | ↖ | ↖ | ↑ | ↖ | ↖ |\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_friction_environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = 0\n",
    "                                           ,stopActionPrice = 0, goalReward = 1000 , punish=-0.01\n",
    "                                           , j_limit = 15, i_limit = 15, p = 0.8, container=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_extreme_friction_environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = -1\n",
    "                                               ,stopActionPrice = -0.8 ,goalReward = 100 , punish=-10\n",
    "                                               ,j_limit = 15, i_limit = 15, p = 0.8, container=None)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "env.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "83fad98a7911d3a2a55c2e5234aea09e74d0252d0d10d90172c6e09f21426bdf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
