{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/zhpinkman/armed-bandit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ./armed-bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mBMoPLmGbrIn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from amalearn.reward import RewardBase\n",
    "from amalearn.agent import AgentBase\n",
    "from amalearn.environment import EnvironmentBase\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pH6sNHxPbrIs"
   },
   "outputs": [],
   "source": [
    "class Environment(EnvironmentBase):\n",
    "    def __init__(self, obstacle = [] ,id = 0, action_count=9, actionPrice = -1, stopActionPrice = -1, goalReward = 100\n",
    "                 , punish=-10, j_limit = 10, i_limit = 10, p = 0.8, container=None):\n",
    "        \"\"\"\n",
    "        initialize your variables\n",
    "        \"\"\"\n",
    "        \n",
    "        self.obstacle = obstacle\n",
    "        \n",
    "        self.x_min = 1\n",
    "        self.x_max = i_limit\n",
    "        \n",
    "        self.y_min = 1\n",
    "        self.y_max = j_limit\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "        self.action_count = action_count\n",
    "        self.actionPrice = actionPrice\n",
    "        self.stopActionPrice = stopActionPrice\n",
    "        self.goalReward = goalReward\n",
    "        self.punish = punish\n",
    "        self.p = p\n",
    "        \n",
    "        action_space = Discrete(action_count)\n",
    "        state_space = Box(low=1, high=max(i_limit, j_limit), shape=(1,2), dtype=int)\n",
    "        \n",
    "        ######################\n",
    "        ###     Action     ###\n",
    "        ###                ###\n",
    "        ### 0 1 2  ↖ ↑ ↗   ###\n",
    "        ### 3 4 5  ← • →   ###\n",
    "        ### 6 7 8  ↙ ↓ ↘  ###\n",
    "        #####################\n",
    "        \n",
    "        #####################\n",
    "        ###     State     ###\n",
    "        ###               ###\n",
    "        ###    (i, j)     ###\n",
    "        #####################\n",
    "        \n",
    "        self.action_list = list(range(1,10))\n",
    "        self.state_list = []\n",
    "        \n",
    "        for i in range(1, i_limit+1):\n",
    "            for j in range(1, j_limit+1):\n",
    "                self.state_list.append(np.array([i, j]))\n",
    "        \n",
    "        super(Environment, self).__init__(action_space=action_space, state_space=state_space, id=id ,container=container)\n",
    "\n",
    "        \n",
    "    def isStatePossible(self, state):\n",
    "        \"\"\"if given state is possible (not out of the grid and not obstacle) return ture\"\"\"\n",
    "        if self.x_min <= state[0] <= self.x_max and self.y_min <= state[1] <= self.y_max:\n",
    "            for obstacle_item in self.obstacle:\n",
    "                if (state==obstacle_item).all():\n",
    "                    return False\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def isAccessible(self, state, state_p):\n",
    "        \"\"\"if given state is Accesible (we can reach state_p by doing an action from state) return true\"\"\"\n",
    "        return abs(state[0]-state_p[0]) <= 1 and abs(state[1] - state_p[1]) <= 1 and self.isStatePossible(state_p)\n",
    "            \n",
    "    def getTransitionStatesAndProbs(self, state, action, state_p):\n",
    "        \"\"\"return probability of transition or T(sp,a,s)\"\"\"\n",
    "        \n",
    "        actions = self.available_actions_state(state)\n",
    "        \n",
    "        if (self.calculate_next_state(state,action)==state_p).all():\n",
    "            if self.isAccessible(state, state_p):\n",
    "                return self.p + (1-self.p) / len(actions)\n",
    "            else:\n",
    "                return self.p\n",
    "        else:\n",
    "            if self.isAccessible(state, state_p):\n",
    "                return (1-self.p) / len(actions)\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "    def getReward(self, state, action, state_p):\n",
    "        \"\"\"return reward of transition\"\"\"\n",
    "        \n",
    "        if action == 4:\n",
    "            action_price = self.stopActionPrice\n",
    "        else:\n",
    "            action_price = self.actionPrice\n",
    "        \n",
    "        if self.terminated_state(state_p):\n",
    "            return action_price + self.goalReward\n",
    "        \n",
    "        if self.isStatePossible(state_p):\n",
    "            return action_price\n",
    "        else:\n",
    "            return action_price + self.punish\n",
    "        \n",
    "    def sample_all_rewards(self):\n",
    "        return \n",
    "    \n",
    "    def calculate_reward(self, action):\n",
    "        return self.getReward(self.current_state, action, self.calculate_next_state(self.current_state, action))\n",
    "\n",
    "    def available_states_state(self, state):\n",
    "        states = []\n",
    "        for i in [-1, 0, +1]:\n",
    "            for j in [-1, 0, +1]:\n",
    "                new_state = np.array([state[0]+i, state[1]+j])\n",
    "                \n",
    "                if self.isAccessible(state, new_state):\n",
    "                    states.append(new_state)\n",
    "        return states\n",
    "    \n",
    "    def terminated(self):\n",
    "        return self.terminated_state(self.current_state)\n",
    "    \n",
    "    def terminated_state(self, state):\n",
    "        return (state==np.array([1,1])).all()\n",
    "        \n",
    "    def observe(self):\n",
    "        return self.current_state \n",
    "\n",
    "    def available_actions(self):\n",
    "        return self.available_actions_state(self.current_state)\n",
    "    \n",
    "    def available_actions_state(self, state):\n",
    "        output_actions = []\n",
    "        for action in range(self.action_count):\n",
    "            next_state = self.calculate_next_state(state, action)\n",
    "            \n",
    "            if self.isAccessible(state, next_state):\n",
    "                output_actions.append(action)\n",
    "        \n",
    "        return output_actions\n",
    "        \n",
    "    \n",
    "    def calculate_next_state(self, state, action):\n",
    "        return np.array([state[0] + (action%3 -1), state[1] + (int(action/3)-1) ])\n",
    "        \n",
    "    def next_state(self, action):\n",
    "        actions = self.available_actions()\n",
    "        \n",
    "        if action not in actions:\n",
    "            actions.append(action)\n",
    "                \n",
    "        probabilities = []\n",
    "                \n",
    "        for action2 in actions:\n",
    "            state2 = self.calculate_next_state(self.current_state, action2)\n",
    "            probabilities.append(self.getTransitionStatesAndProbs(self.current_state, action, state2))\n",
    "        \n",
    "        final_action = random.choices(population=actions, weights=probabilities, k=1)[0]\n",
    "        \n",
    "        real_next_state = self.calculate_next_state(self.current_state, final_action)\n",
    "        \n",
    "        if not self.isStatePossible(real_next_state):\n",
    "            real_next_state = self.current_state\n",
    "        \n",
    "        self.last_action = action\n",
    "        \n",
    "        self.sliped = not (final_action==action)\n",
    "        \n",
    "        self.current_state = real_next_state\n",
    "        \n",
    "        return\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = np.array([15, 15])\n",
    "        \n",
    "        self.last_action = None\n",
    "        self.sliped = None\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"{self.current_state} \\t {self.last_action} \\t {self.sliped}\")\n",
    "        return \n",
    "\n",
    "    def close(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_states =  [np.array([7, 1]), np.array([8, 1]), np.array([7, 2]), np.array([8, 2])\n",
    "                ,np.array([7, 3]), np.array([8, 3]), np.array([7, 4]), np.array([8, 4])\n",
    "                ,np.array([13, 8]), np.array([14, 8]), np.array([15, 8])\n",
    "                ,np.array([13, 9]), np.array([14, 9]), np.array([15, 9])\n",
    "                ,np.array([6, 12]), np.array([7, 12]), np.array([6, 13]), np.array([7, 13])\n",
    "                ,np.array([6, 14]), np.array([7, 14]), np.array([6, 15]), np.array([7, 15])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "898Jlhsycyes"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Agent(AgentBase):\n",
    "    def __init__(self, id, environment, discount, theta):\n",
    "        self.environment = environment\n",
    "        \n",
    "        # init V\n",
    "        self.V = {}\n",
    "        \n",
    "        # init policy\n",
    "        self.policy = {}\n",
    "        \n",
    "        super(Agent, self).__init__(id, environment)\n",
    "        \n",
    "        self.discount = discount\n",
    "        \n",
    "        self.theta = theta\n",
    "\n",
    "        # initialize a random policy and V(s) = 0 for each state        \n",
    "        self.value_initialization()\n",
    "        \n",
    "        self.policy_initialization()\n",
    "    \n",
    "    def value_initialization(self):\n",
    "        for state in self.environment.state_list:\n",
    "            self.V[tuple(state)] = 0\n",
    "        \n",
    "    def policy_initialization(self):\n",
    "        for state in self.environment.state_list:\n",
    "            self.policy[tuple(state)] = random.choice(self.environment.action_list)\n",
    "        \n",
    "    def policy_evaluation(self):\n",
    "        new_V = {}\n",
    "\n",
    "        delta = 0\n",
    "        for state in self.environment.state_list:\n",
    "            \n",
    "            available_states  = self.environment.available_states_state(state)\n",
    "\n",
    "            action = self.policy[tuple(state)]\n",
    "            \n",
    "            sum = 0\n",
    "            \n",
    "            for state_p in available_states:\n",
    "                p_sp = self.environment.getTransitionStatesAndProbs(state, action, state_p)\n",
    "                r_sp = self.environment.getReward(state, action, state_p)\n",
    "                v_sp = self.V[tuple(state_p)]\n",
    "\n",
    "                sum += p_sp * (r_sp + self.discount * v_sp)\n",
    "\n",
    "            new_V[tuple(state)] = sum\n",
    "            delta = max(delta, abs(self.V[tuple(state)] - new_V[tuple(state)]))\n",
    "        \n",
    "        print(f\"delta = {round(delta, 2)}\")\n",
    "        self.V = deepcopy(new_V)\n",
    "\n",
    "        if delta < self.theta[\"delta_treshold\"]:\n",
    "            print(\"Value convergenced\")\n",
    "\n",
    "        return delta < self.theta[\"delta_treshold\"]\n",
    "    \n",
    "    def policy_improvement(self):\n",
    "        policy_stable = True\n",
    "        \n",
    "        new_policy = {}\n",
    "        \n",
    "        for state in self.environment.state_list:\n",
    "\n",
    "            available_actions = self.environment.available_actions_state(state)\n",
    "            available_states  = self.environment.available_states_state(state)\n",
    "            \n",
    "            max_value = -math.inf\n",
    "            argmax = None\n",
    "            \n",
    "            for action in available_actions:\n",
    "                sum = 0\n",
    "                for state_p in available_states:\n",
    "                    p_sp = self.environment.getTransitionStatesAndProbs(state, action, state_p)\n",
    "                    v_sp = self.V[tuple(state_p)]\n",
    "\n",
    "                    sum += p_sp * v_sp\n",
    "            \n",
    "                if sum > max_value:\n",
    "                    max_value = sum\n",
    "                    argmax = action\n",
    "                    \n",
    "            new_policy[tuple(state)] = argmax\n",
    "            \n",
    "            if self.policy[tuple(state)] != argmax:\n",
    "                policy_stable = False\n",
    "        \n",
    "        self.policy = deepcopy(new_policy)\n",
    "        if policy_stable:\n",
    "            print(\"Policy convergenced\")\n",
    "            \n",
    "        return policy_stable\n",
    "    \n",
    "    \n",
    "    def policy_iteration(self):\n",
    "        for iter in range(self.theta[\"max_iter\"]):\n",
    "            print(f\"iter = {iter} -> \", end=\"\")\n",
    "\n",
    "            flag_value = self.policy_evaluation()\n",
    "\n",
    "            flag_policy = self.policy_improvement()\n",
    "\n",
    "            if flag_value or flag_policy:\n",
    "                break\n",
    "                \n",
    "    def value_iteration(self):\n",
    "        for iter in range(self.theta[\"max_iter\"]):\n",
    "            new_V = {}\n",
    "\n",
    "            delta = 0\n",
    "            for state in self.environment.state_list:\n",
    "                new_V[tuple(state)] = -math.inf\n",
    "\n",
    "                available_actions = self.environment.available_actions_state(state)\n",
    "                available_states  = self.environment.available_states_state(state)\n",
    "\n",
    "                for action in available_actions:\n",
    "                    sum = 0\n",
    "                    for state_p in available_states:\n",
    "                        p_sp = self.environment.getTransitionStatesAndProbs(state, action, state_p)\n",
    "                        r_sp = self.environment.getReward(state, action, state_p)\n",
    "                        v_sp = self.V[tuple(state_p)]\n",
    "\n",
    "                        sum += p_sp * (r_sp + self.discount * v_sp)\n",
    "\n",
    "                    new_V[tuple(state)] = max(new_V[tuple(state)], sum)\n",
    "                delta = max(delta, abs(self.V[tuple(state)] - new_V[tuple(state)]))\n",
    "                \n",
    "            print(f\"iter = {iter} -> delta = {round(delta, 2)}\")\n",
    "            self.V = deepcopy(new_V)\n",
    "\n",
    "            if delta < self.theta[\"delta_treshold\"]:\n",
    "                print(\"Value convergenced\")\n",
    "                break\n",
    "        \n",
    "        self.policy_extraction()\n",
    "    \n",
    "    def policy_extraction(self):\n",
    "        for state in self.environment.state_list:\n",
    "\n",
    "            available_actions = self.environment.available_actions_state(state)\n",
    "            available_states  = self.environment.available_states_state(state)\n",
    "            \n",
    "            max_value = -math.inf\n",
    "            argmax = None\n",
    "            \n",
    "            for action in available_actions:\n",
    "                sum = 0\n",
    "                for state_p in available_states:\n",
    "                    p_sp = self.environment.getTransitionStatesAndProbs(state, action, state_p)\n",
    "                    v_sp = self.V[tuple(state_p)]\n",
    "\n",
    "                    sum += p_sp * v_sp\n",
    "            \n",
    "                if sum > max_value:\n",
    "                    max_value = sum\n",
    "                    argmax = action\n",
    "                    \n",
    "            self.policy[tuple(state)] = argmax\n",
    "    \n",
    "    def print_value(self):\n",
    "        p = PrettyTable()\n",
    "\n",
    "        for j in range(1,16):\n",
    "            row = []\n",
    "            for i in range(1, 16):\n",
    "                if self.environment.isStatePossible(np.array([i,j])):\n",
    "                    row.append(int(self.V[(i,j)]))\n",
    "                else:\n",
    "                    row.append(\"###\")\n",
    "                \n",
    "            p.add_row(row)\n",
    "\n",
    "        print (p.get_string(header=False, border=True))\n",
    "    \n",
    "    def print_policy(self):\n",
    "        p = PrettyTable()\n",
    "\n",
    "        for j in range(1,16):\n",
    "            row = []\n",
    "            for i in range(1, 16):\n",
    "                if self.environment.isStatePossible(np.array([i,j])):\n",
    "                    row.append(self.action_symbol(int(self.policy[(i,j)])))\n",
    "                else:\n",
    "                    row.append(\"▮\")\n",
    "                \n",
    "                \n",
    "            p.add_row(row)\n",
    "\n",
    "        print (p.get_string(header=False, border=True))\n",
    "    \n",
    "    def take_action(self) -> (object, float, bool, object):\n",
    "        # observation, reward, done, info\n",
    "        return self.environment.step(random.choice(self.environment.action_list))\n",
    "    \n",
    "    def action_symbol(self, action):\n",
    "        if action == 0:\n",
    "            return \"↖\"\n",
    "        elif action == 1:\n",
    "            return \"↑\"\n",
    "        elif action == 2:\n",
    "            return \"↗\"\n",
    "        elif action == 3:\n",
    "            return \"←\"\n",
    "        elif action == 4:\n",
    "            return \"•\"\n",
    "        elif action == 5:\n",
    "            return \"→\"\n",
    "        elif action == 6:\n",
    "            return \"↙\"\n",
    "        elif action == 7:\n",
    "            return \"↓\"\n",
    "        elif action == 8:\n",
    "            return \"↘\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0 -> delta = 49.99\n",
      "iter = 1 -> delta = 842.24\n",
      "iter = 2 -> delta = 757.67\n",
      "iter = 3 -> delta = 673.83\n",
      "iter = 4 -> delta = 605.79\n",
      "iter = 5 -> delta = 544.38\n",
      "iter = 6 -> delta = 489.79\n",
      "iter = 7 -> delta = 440.7\n",
      "iter = 8 -> delta = 396.6\n",
      "iter = 9 -> delta = 356.93\n",
      "iter = 10 -> delta = 321.23\n",
      "iter = 11 -> delta = 289.1\n",
      "iter = 12 -> delta = 260.19\n",
      "iter = 13 -> delta = 234.17\n",
      "iter = 14 -> delta = 210.75\n",
      "iter = 15 -> delta = 189.68\n",
      "iter = 16 -> delta = 170.71\n",
      "Policy convergenced\n",
      "Optimal Value\n",
      "+------+------+------+------+------+------+------+------+------+------+------+-----+-----+-----+-----+\n",
      "| 6833 | 6755 | 5750 | 4869 | 4098 | 3479 | ###  | ###  | 766  | 765  | 750  | 707 | 501 | 316 | 178 |\n",
      "| 6755 | 6701 | 5727 | 4865 | 4097 | 3479 | ###  | ###  | 1038 | 1030 | 1000 | 748 | 518 | 319 | 179 |\n",
      "| 5750 | 5727 | 5628 | 4826 | 4090 | 3475 | ###  | ###  | 1382 | 1365 | 1053 | 771 | 522 | 321 | 179 |\n",
      "| 4868 | 4865 | 4826 | 4701 | 4038 | 3437 | ###  | ###  | 1795 | 1412 | 1076 | 775 | 523 | 321 | 181 |\n",
      "| 4094 | 4093 | 4086 | 4035 | 3900 | 3358 | 2840 | 2294 | 1827 | 1430 | 1078 | 776 | 524 | 324 | 185 |\n",
      "| 3413 | 3413 | 3411 | 3400 | 3344 | 3205 | 2751 | 2302 | 1836 | 1431 | 1079 | 777 | 528 | 331 | 186 |\n",
      "| 2815 | 2815 | 2814 | 2811 | 2797 | 2738 | 2601 | 2219 | 1830 | 1429 | 1079 | 783 | 539 | 332 | 186 |\n",
      "| 2289 | 2289 | 2289 | 2288 | 2284 | 2268 | 2207 | 2077 | 1754 | 1419 | 1076 | 790 | ### | ### | ### |\n",
      "| 1828 | 1828 | 1828 | 1828 | 1827 | 1821 | 1803 | 1744 | 1622 | 1349 | 1062 | 785 | ### | ### | ### |\n",
      "| 1424 | 1424 | 1424 | 1424 | 1424 | 1423 | 1417 | 1398 | 1341 | 1229 | 999  | 763 | 533 | 327 | 183 |\n",
      "| 1073 | 1073 | 1073 | 1074 | 1081 | 1090 | 1089 | 1072 | 1046 | 992  | 892  | 702 | 508 | 325 | 182 |\n",
      "| 772  | 772  | 773  | 779  | 792  | ###  | ###  | 790  | 769  | 745  | 697  | 611 | 458 | 306 | 180 |\n",
      "| 520  | 521  | 525  | 535  | 536  | ###  | ###  | 533  | 532  | 516  | 495  | 454 | 385 | 269 | 166 |\n",
      "| 319  | 322  | 329  | 329  | 330  | ###  | ###  | 328  | 328  | 326  | 314  | 297 | 266 | 215 | 139 |\n",
      "| 180  | 184  | 184  | 185  | 185  | ###  | ###  | 184  | 184  | 183  | 181  | 173 | 161 | 138 | 108 |\n",
      "+------+------+------+------+------+------+------+------+------+------+------+-----+-----+-----+-----+\n",
      "\n",
      "Optimal Policy\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "| • | ← | ← | ← | ← | ← | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← | ← | ← | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↙ | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← |\n",
      "| ↗ | ↗ | ↗ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↗ | ▮ | ▮ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "base_environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = -0.01, stopActionPrice = -0.01\n",
    "                               ,goalReward = 1000, punish=-1, j_limit = 15, i_limit = 15, p = 0.8, container=None)\n",
    "\n",
    "theta = {\"max_iter\": 50, \"delta_treshold\": 0.001}\n",
    "agent = Agent(id=0, environment=base_environment, discount=0.9, theta=theta)\n",
    "\n",
    "agent.policy_iteration()\n",
    "\n",
    "print(\"Optimal Value\")\n",
    "agent.print_value()\n",
    "\n",
    "print(\"\\nOptimal Policy\")\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0 -> delta = 833.33\n",
      "iter = 1 -> delta = 642.25\n",
      "iter = 2 -> delta = 787.22\n",
      "Policy convergenced\n",
      "Optimal Value\n",
      "+------+------+-----+-----+---+-----+-----+-----+---+---+---+---+-----+-----+-----+\n",
      "| 1467 | 1448 | 552 | 487 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "| 1412 | 1399 | 528 | 475 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "| 516  | 516  | 487 | 456 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  20  |  19  |  18 |  12 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0   |  0   |  0  |  0  | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0   |  0   |  0  |  0  | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0   |  0   |  0  |  0  | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0   |  0   |  0  |  0  | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 | ### | ### | ### |\n",
      "|  0   |  0   |  0  |  0  | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 | ### | ### | ### |\n",
      "|  0   |  0   |  0  |  0  | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0   |  0   |  0  |  0  | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0   |  0   |  0  |  0  | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0   |  0   |  0  |  0  | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0   |  0   |  0  |  0  | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0   |  0   |  0  |  0  | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "+------+------+-----+-----+---+-----+-----+-----+---+---+---+---+-----+-----+-----+\n",
      "\n",
      "Optimal Policy\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "| • | ← | ← | ← | ← | ← | ▮ | ▮ | • | ← | ← | ← | ← | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↗ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "without_friction_environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = 0\n",
    "                                           ,stopActionPrice = 0, goalReward = 1000 , punish=-0.01\n",
    "                                           , j_limit = 15, i_limit = 15, p = 0.8, container=None)\n",
    "\n",
    "theta = {\"max_iter\": 50, \"delta_treshold\": 0.001}\n",
    "agent = Agent(id=0, environment=without_friction_environment, discount=0.9, theta=theta)\n",
    "\n",
    "agent.policy_iteration()\n",
    "\n",
    "print(\"Optimal Value\")\n",
    "agent.print_value()\n",
    "\n",
    "print(\"\\nOptimal Policy\")\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0 -> delta = 4.0\n",
      "iter = 1 -> delta = 83.56\n",
      "iter = 2 -> delta = 75.11\n",
      "iter = 3 -> delta = 66.79\n",
      "iter = 4 -> delta = 60.04\n",
      "iter = 5 -> delta = 53.96\n",
      "iter = 6 -> delta = 48.54\n",
      "iter = 7 -> delta = 43.68\n",
      "iter = 8 -> delta = 39.31\n",
      "iter = 9 -> delta = 35.38\n",
      "iter = 10 -> delta = 31.84\n",
      "iter = 11 -> delta = 28.65\n",
      "iter = 12 -> delta = 25.79\n",
      "iter = 13 -> delta = 23.21\n",
      "iter = 14 -> delta = 20.89\n",
      "Policy convergenced\n",
      "Optimal Value\n",
      "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+----+----+-----+-----+-----+\n",
      "| 640 | 632 | 532 | 443 | 366 | 304 | ### | ### |  38 |  38 | 36 | 33 |  16 |  3  |  -3 |\n",
      "| 632 | 627 | 529 | 443 | 366 | 304 | ### | ### |  63 |  62 | 59 | 36 |  17 |  3  |  -3 |\n",
      "| 532 | 529 | 519 | 439 | 365 | 304 | ### | ### |  95 |  94 | 64 | 38 |  18 |  3  |  -3 |\n",
      "| 443 | 443 | 439 | 426 | 360 | 300 | ### | ### | 136 |  98 | 66 | 39 |  18 |  3  |  -3 |\n",
      "| 366 | 366 | 365 | 360 | 346 | 292 | 240 | 185 | 139 | 100 | 66 | 39 |  18 |  3  |  -3 |\n",
      "| 297 | 297 | 297 | 296 | 290 | 277 | 231 | 186 | 140 | 100 | 66 | 39 |  18 |  4  |  -3 |\n",
      "| 238 | 238 | 238 | 237 | 236 | 230 | 216 | 178 | 139 | 100 | 66 | 39 |  19 |  4  |  -3 |\n",
      "| 185 | 185 | 185 | 185 | 184 | 183 | 177 | 164 | 132 |  99 | 66 | 40 | ### | ### | ### |\n",
      "| 139 | 139 | 139 | 139 | 139 | 138 | 137 | 131 | 119 |  92 | 65 | 39 | ### | ### | ### |\n",
      "|  99 |  99 |  99 |  99 |  99 |  99 |  99 |  97 |  91 |  81 | 59 | 37 |  18 |  4  |  -3 |\n",
      "|  66 |  66 |  66 |  66 |  67 |  67 |  67 |  66 |  63 |  58 | 49 | 32 |  17 |  3  |  -3 |\n",
      "|  38 |  38 |  38 |  39 |  40 | ### | ### |  40 |  38 |  36 | 32 | 25 |  13 |  3  |  -3 |\n",
      "|  17 |  18 |  18 |  18 |  18 | ### | ### |  18 |  18 |  17 | 16 | 13 |  8  |  1  |  -4 |\n",
      "|  3  |  3  |  4  |  4  |  4  | ### | ### |  4  |  4  |  4  | 3  | 2  |  1  |  -1 |  -4 |\n",
      "|  -3 |  -3 |  -3 |  -3 |  -3 | ### | ### |  -3 |  -3 |  -3 | -3 | -3 |  -4 |  -4 |  -5 |\n",
      "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+----+----+-----+-----+-----+\n",
      "\n",
      "Optimal Policy\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "| • | ← | ← | ← | ← | ← | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← | ← | ← | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↙ | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← |\n",
      "| ↑ | ↖ | ↗ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↗ | ▮ | ▮ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "with_extreme_friction_environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = -1\n",
    "                                               ,stopActionPrice = -0.8 ,goalReward = 100 , punish=-10\n",
    "                                               ,j_limit = 15, i_limit = 15, p = 0.8, container=None)\n",
    "\n",
    "theta = {\"max_iter\": 50, \"delta_treshold\": 3}\n",
    "agent = Agent(id=0, environment=with_extreme_friction_environment, discount=0.9, theta=theta)\n",
    "\n",
    "agent.policy_iteration()\n",
    "\n",
    "print(\"Optimal Value\")\n",
    "agent.print_value()\n",
    "\n",
    "print(\"\\nOptimal Policy\")\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0 -> delta = 849.99\n",
      "iter = 1 -> delta = 1333.46\n",
      "iter = 2 -> delta = 867.61\n",
      "iter = 3 -> delta = 564.09\n",
      "iter = 4 -> delta = 367.09\n",
      "iter = 5 -> delta = 248.23\n",
      "iter = 6 -> delta = 186.17\n",
      "iter = 7 -> delta = 122.42\n",
      "iter = 8 -> delta = 91.81\n",
      "iter = 9 -> delta = 66.3\n",
      "iter = 10 -> delta = 49.67\n",
      "iter = 11 -> delta = 36.98\n",
      "iter = 12 -> delta = 27.73\n",
      "iter = 13 -> delta = 20.77\n",
      "iter = 14 -> delta = 15.58\n",
      "Policy convergenced\n",
      "Optimal Value\n",
      "+------+------+------+------+------+-----+-----+-----+-----+-----+-----+----+-----+-----+-----+\n",
      "| 3317 | 3252 | 2296 | 1618 | 1137 | 817 | ### | ### |  62 |  62 |  60 | 56 |  32 |  14 |  5  |\n",
      "| 3252 | 3207 | 2278 | 1616 | 1137 | 817 | ### | ### | 101 | 100 |  96 | 60 |  33 |  15 |  5  |\n",
      "| 2296 | 2278 | 2211 | 1592 | 1133 | 816 | ### | ### | 161 | 158 | 103 | 63 |  34 |  15 |  5  |\n",
      "| 1618 | 1616 | 1592 | 1522 | 1108 | 802 | ### | ### | 248 | 165 | 106 | 63 |  34 |  15 |  5  |\n",
      "| 1136 | 1136 | 1132 | 1107 | 1045 | 771 | 558 | 379 | 254 | 168 | 107 | 63 |  34 |  15 |  5  |\n",
      "| 794  | 794  | 793  | 789  | 766  | 714 | 529 | 380 | 256 | 169 | 107 | 63 |  34 |  15 |  5  |\n",
      "| 551  | 551  | 550  | 549  | 545  | 526 | 484 | 359 | 255 | 168 | 107 | 64 |  35 |  15 |  5  |\n",
      "| 378  | 378  | 378  | 377  | 376  | 372 | 356 | 324 | 239 | 167 | 106 | 65 | ### | ### | ### |\n",
      "| 255  | 255  | 255  | 255  | 255  | 253 | 250 | 237 | 212 | 154 | 104 | 64 | ### | ### | ### |\n",
      "| 168  | 168  | 168  | 168  | 168  | 167 | 166 | 163 | 153 | 135 |  95 | 62 |  35 |  15 |  5  |\n",
      "| 106  | 106  | 106  | 106  | 107  | 108 | 108 | 106 | 102 |  95 |  81 | 55 |  32 |  15 |  5  |\n",
      "|  63  |  63  |  63  |  63  |  65  | ### | ### |  65 |  62 |  60 |  54 | 45 |  28 |  14 |  5  |\n",
      "|  34  |  34  |  34  |  35  |  35  | ### | ### |  35 |  35 |  33 |  31 | 28 |  22 |  11 |  4  |\n",
      "|  14  |  15  |  15  |  15  |  15  | ### | ### |  15 |  15 |  15 |  14 | 13 |  11 |  8  |  3  |\n",
      "|  5   |  5   |  5   |  5   |  5   | ### | ### |  5  |  5  |  5  |  5  | 5  |  4  |  3  |  2  |\n",
      "+------+------+------+------+------+-----+-----+-----+-----+-----+-----+----+-----+-----+-----+\n",
      "\n",
      "Optimal Policy\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "| • | ← | ← | ← | ← | ← | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← | ← | ← | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↙ | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← |\n",
      "| ↑ | ↖ | ↗ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↗ | ▮ | ▮ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "base_environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = -0.01, stopActionPrice = -0.01\n",
    "                               ,goalReward = 1000, punish=-1, j_limit = 15, i_limit = 15, p = 0.8, container=None)\n",
    "\n",
    "theta = {\"max_iter\": 50, \"delta_treshold\": 0.001}\n",
    "agent = Agent(id=0, environment=base_environment, discount=0.75, theta=theta)\n",
    "\n",
    "agent.policy_iteration()\n",
    "\n",
    "print(\"Optimal Value\")\n",
    "agent.print_value()\n",
    "\n",
    "print(\"\\nOptimal Policy\")\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0 -> delta = 849.99\n",
      "iter = 1 -> delta = 1155.64\n",
      "iter = 2 -> delta = 501.16\n",
      "iter = 3 -> delta = 217.16\n",
      "iter = 4 -> delta = 94.19\n",
      "iter = 5 -> delta = 42.44\n",
      "iter = 6 -> delta = 21.22\n",
      "iter = 7 -> delta = 8.48\n",
      "iter = 8 -> delta = 4.14\n",
      "iter = 9 -> delta = 1.98\n",
      "iter = 10 -> delta = 0.96\n",
      "iter = 11 -> delta = 0.48\n",
      "iter = 12 -> delta = 0.24\n",
      "iter = 13 -> delta = 0.12\n",
      "iter = 14 -> delta = 0.06\n",
      "Policy convergenced\n",
      "Optimal Value\n",
      "+------+------+-----+-----+-----+-----+-----+-----+---+---+---+---+-----+-----+-----+\n",
      "| 1691 | 1644 | 748 | 340 | 155 |  73 | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "| 1644 | 1612 | 737 | 339 | 154 |  73 | ### | ### | 1 | 1 | 1 | 0 |  0  |  0  |  0  |\n",
      "| 748  | 737  | 705 | 330 | 154 |  73 | ### | ### | 2 | 2 | 1 | 0 |  0  |  0  |  0  |\n",
      "| 340  | 339  | 330 | 309 | 148 |  71 | ### | ### | 6 | 2 | 1 | 0 |  0  |  0  |  0  |\n",
      "| 155  | 154  | 154 | 148 | 135 |  66 |  32 |  14 | 6 | 2 | 1 | 0 |  0  |  0  |  0  |\n",
      "|  70  |  70  |  70 |  69 |  66 |  59 |  29 |  14 | 6 | 2 | 1 | 0 |  0  |  0  |  0  |\n",
      "|  32  |  32  |  32 |  31 |  31 |  29 |  26 |  13 | 6 | 2 | 1 | 0 |  0  |  0  |  0  |\n",
      "|  14  |  14  |  14 |  14 |  14 |  14 |  13 |  11 | 5 | 2 | 1 | 0 | ### | ### | ### |\n",
      "|  6   |  6   |  6  |  6  |  6  |  6  |  6  |  5  | 5 | 2 | 1 | 0 | ### | ### | ### |\n",
      "|  2   |  2   |  2  |  2  |  2  |  2  |  2  |  2  | 2 | 2 | 1 | 0 |  0  |  0  |  0  |\n",
      "|  1   |  1   |  1  |  1  |  1  |  1  |  1  |  1  | 1 | 1 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0   |  0   |  0  |  0  |  0  | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0   |  0   |  0  |  0  |  0  | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0   |  0   |  0  |  0  |  0  | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0   |  0   |  0  |  0  |  0  | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "+------+------+-----+-----+-----+-----+-----+-----+---+---+---+---+-----+-----+-----+\n",
      "\n",
      "Optimal Policy\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "| • | ← | ← | ← | ← | ← | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← | ← | ← | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↙ | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↗ | ▮ | ▮ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "base_environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = -0.01, stopActionPrice = -0.01\n",
    "                               ,goalReward = 1000, punish=-1, j_limit = 15, i_limit = 15, p = 0.8, container=None)\n",
    "\n",
    "theta = {\"max_iter\": 50, \"delta_treshold\": 0.001}\n",
    "agent = Agent(id=0, environment=base_environment, discount=0.5, theta=theta)\n",
    "\n",
    "agent.policy_iteration()\n",
    "\n",
    "print(\"Optimal Value\")\n",
    "agent.print_value()\n",
    "\n",
    "print(\"\\nOptimal Policy\")\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0 -> delta = 833.32\n",
      "iter = 1 -> delta = 730.2\n",
      "iter = 2 -> delta = 742.21\n",
      "iter = 3 -> delta = 74.06\n",
      "iter = 4 -> delta = 7.31\n",
      "iter = 5 -> delta = 0.73\n",
      "iter = 6 -> delta = 0.07\n",
      "iter = 7 -> delta = 0.01\n",
      "iter = 8 -> delta = 0.0\n",
      "Value convergenced\n",
      "Optimal Value\n",
      "+-----+-----+----+---+---+-----+-----+-----+---+---+---+---+-----+-----+-----+\n",
      "| 943 | 921 | 80 | 7 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "| 921 | 906 | 78 | 6 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  80 |  78 | 75 | 6 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  7  |  6  | 6  | 6 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 | ### | ### | ### |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 | ### | ### | ### |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "+-----+-----+----+---+---+-----+-----+-----+---+---+---+---+-----+-----+-----+\n",
      "\n",
      "Optimal Policy\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "| • | ← | ← | ← | ← | ← | ▮ | ▮ | • | ← | ← | ← | ← | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↓ | ↙ | ↙ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↓ | ↙ | ↙ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↙ | ↙ | ↙ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ↙ | ↙ | ↙ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← | ← | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↗ | ↑ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↗ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↙ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↓ | ↙ | ↙ | ↙ | ↗ | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↓ | ↙ | ↙ | ↙ | ↙ | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↓ | ↙ | ↙ | ↙ | ↙ | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| • | ← | ← | ← | ← | ▮ | ▮ | • | ← | ← | ← | ← | ← | ← | ← |\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "base_environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = -0.01, stopActionPrice = -0.01\n",
    "                               ,goalReward = 1000, punish=-1, j_limit = 15, i_limit = 15, p = 0.8, container=None)\n",
    "\n",
    "theta = {\"max_iter\": 50, \"delta_treshold\": 0.001}\n",
    "agent = Agent(id=0, environment=base_environment, discount=0.1, theta=theta)\n",
    "\n",
    "agent.policy_iteration()\n",
    "\n",
    "print(\"Optimal Value\")\n",
    "agent.print_value()\n",
    "\n",
    "print(\"\\nOptimal Policy\")\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0 -> delta = 50.0\n",
      "iter = 1 -> delta = 846.48\n",
      "Policy convergenced\n",
      "Optimal Value\n",
      "+-----+-----+----+---+---+-----+-----+-----+---+---+---+---+-----+-----+-----+\n",
      "| 896 | 877 | 28 | 0 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "| 877 | 864 | 27 | 0 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  28 |  27 | 18 | 0 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 | ### | ### | ### |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 | ### | ### | ### |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "+-----+-----+----+---+---+-----+-----+-----+---+---+---+---+-----+-----+-----+\n",
      "\n",
      "Optimal Policy\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "| • | ← | ← | ← | ← | ← | ▮ | ▮ | • | ← | ← | ← | ← | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↗ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "without_friction_environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = 0\n",
    "                                           ,stopActionPrice = 0, goalReward = 1000 , punish=-0.01\n",
    "                                           , j_limit = 15, i_limit = 15, p = 0.8, container=None)\n",
    "\n",
    "theta = {\"max_iter\": 50, \"delta_treshold\": 0.001}\n",
    "agent = Agent(id=0, environment=without_friction_environment, discount=0.99, theta=theta)\n",
    "\n",
    "agent.policy_iteration()\n",
    "\n",
    "print(\"Optimal Value\")\n",
    "agent.print_value()\n",
    "\n",
    "print(\"\\nOptimal Policy\")\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0 -> delta = 50.0\n",
      "iter = 1 -> delta = 835.21\n",
      "Policy convergenced\n",
      "Optimal Value\n",
      "+-----+-----+----+---+---+-----+-----+-----+---+---+---+---+-----+-----+-----+\n",
      "| 885 | 866 | 21 | 0 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "| 866 | 854 | 20 | 0 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  21 |  20 | 13 | 0 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 | ### | ### | ### |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 | ### | ### | ### |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "+-----+-----+----+---+---+-----+-----+-----+---+---+---+---+-----+-----+-----+\n",
      "\n",
      "Optimal Policy\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "| • | ← | ← | ← | ← | ← | ▮ | ▮ | • | ← | ← | ← | ← | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↗ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "without_friction_environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = 0\n",
    "                                           ,stopActionPrice = 0, goalReward = 1000 , punish=-0.01\n",
    "                                           , j_limit = 15, i_limit = 15, p = 0.8, container=None)\n",
    "\n",
    "theta = {\"max_iter\": 50, \"delta_treshold\": 0.001}\n",
    "agent = Agent(id=0, environment=without_friction_environment, discount=0.75, theta=theta)\n",
    "\n",
    "agent.policy_iteration()\n",
    "\n",
    "print(\"Optimal Value\")\n",
    "agent.print_value()\n",
    "\n",
    "print(\"\\nOptimal Policy\")\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0 -> delta = 50.0\n",
      "iter = 1 -> delta = 823.47\n",
      "Policy convergenced\n",
      "Optimal Value\n",
      "+-----+-----+----+---+---+-----+-----+-----+---+---+---+---+-----+-----+-----+\n",
      "| 873 | 855 | 14 | 0 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "| 855 | 843 | 13 | 0 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  14 |  13 | 9  | 0 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 | ### | ### | ### |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 | ### | ### | ### |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0  | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "+-----+-----+----+---+---+-----+-----+-----+---+---+---+---+-----+-----+-----+\n",
      "\n",
      "Optimal Policy\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "| • | ← | ← | ← | ← | ← | ▮ | ▮ | • | ← | ← | ← | ← | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↗ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "without_friction_environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = 0\n",
    "                                           ,stopActionPrice = 0, goalReward = 1000 , punish=-0.01\n",
    "                                           , j_limit = 15, i_limit = 15, p = 0.8, container=None)\n",
    "\n",
    "theta = {\"max_iter\": 50, \"delta_treshold\": 0.001}\n",
    "agent = Agent(id=0, environment=without_friction_environment, discount=0.5, theta=theta)\n",
    "\n",
    "agent.policy_iteration()\n",
    "\n",
    "print(\"Optimal Value\")\n",
    "agent.print_value()\n",
    "\n",
    "print(\"\\nOptimal Policy\")\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0 -> delta = 50.0\n",
      "iter = 1 -> delta = 804.69\n",
      "Policy convergenced\n",
      "Optimal Value\n",
      "+-----+-----+---+---+---+-----+-----+-----+---+---+---+---+-----+-----+-----+\n",
      "| 854 | 837 | 2 | 0 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "| 837 | 826 | 2 | 0 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  2  |  2  | 1 | 0 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0 | 0 | 0 |  0  | ### | ### | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0 | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0 | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0 | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0 | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 | ### | ### | ### |\n",
      "|  0  |  0  | 0 | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 | ### | ### | ### |\n",
      "|  0  |  0  | 0 | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0 | 0 | 0 |  0  |  0  |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0 | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0 | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0 | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "|  0  |  0  | 0 | 0 | 0 | ### | ### |  0  | 0 | 0 | 0 | 0 |  0  |  0  |  0  |\n",
      "+-----+-----+---+---+---+-----+-----+-----+---+---+---+---+-----+-----+-----+\n",
      "\n",
      "Optimal Policy\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "| • | ← | ← | ← | ← | ← | ▮ | ▮ | • | ← | ← | ← | ← | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↗ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "without_friction_environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = 0\n",
    "                                           ,stopActionPrice = 0, goalReward = 1000 , punish=-0.01\n",
    "                                           , j_limit = 15, i_limit = 15, p = 0.8, container=None)\n",
    "\n",
    "theta = {\"max_iter\": 50, \"delta_treshold\": 0.001}\n",
    "agent = Agent(id=0, environment=without_friction_environment, discount=0.1, theta=theta)\n",
    "\n",
    "agent.policy_iteration()\n",
    "\n",
    "print(\"Optimal Value\")\n",
    "agent.print_value()\n",
    "\n",
    "print(\"\\nOptimal Policy\")\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0 -> delta = 849.99\n",
      "iter = 1 -> delta = 762.24\n",
      "iter = 2 -> delta = 677.75\n",
      "iter = 3 -> delta = 609.21\n",
      "iter = 4 -> delta = 547.43\n",
      "iter = 5 -> delta = 492.53\n",
      "iter = 6 -> delta = 443.16\n",
      "iter = 7 -> delta = 398.81\n",
      "iter = 8 -> delta = 358.92\n",
      "iter = 9 -> delta = 323.02\n",
      "iter = 10 -> delta = 290.71\n",
      "iter = 11 -> delta = 261.64\n",
      "iter = 12 -> delta = 235.48\n",
      "iter = 13 -> delta = 211.93\n",
      "iter = 14 -> delta = 190.74\n",
      "iter = 15 -> delta = 171.66\n",
      "iter = 16 -> delta = 154.5\n",
      "iter = 17 -> delta = 139.05\n",
      "iter = 18 -> delta = 125.14\n",
      "iter = 19 -> delta = 112.63\n",
      "iter = 20 -> delta = 101.36\n",
      "iter = 21 -> delta = 91.23\n",
      "iter = 22 -> delta = 82.11\n",
      "iter = 23 -> delta = 73.89\n",
      "iter = 24 -> delta = 66.51\n",
      "iter = 25 -> delta = 59.85\n",
      "iter = 26 -> delta = 53.87\n",
      "iter = 27 -> delta = 48.48\n",
      "iter = 28 -> delta = 43.63\n",
      "iter = 29 -> delta = 39.27\n",
      "iter = 30 -> delta = 35.34\n",
      "iter = 31 -> delta = 31.81\n",
      "iter = 32 -> delta = 28.63\n",
      "iter = 33 -> delta = 25.77\n",
      "iter = 34 -> delta = 23.19\n",
      "iter = 35 -> delta = 20.87\n",
      "iter = 36 -> delta = 18.78\n",
      "iter = 37 -> delta = 16.9\n",
      "iter = 38 -> delta = 15.21\n",
      "iter = 39 -> delta = 13.69\n",
      "iter = 40 -> delta = 12.32\n",
      "iter = 41 -> delta = 11.09\n",
      "iter = 42 -> delta = 9.98\n",
      "iter = 43 -> delta = 8.98\n",
      "iter = 44 -> delta = 8.09\n",
      "iter = 45 -> delta = 7.28\n",
      "iter = 46 -> delta = 6.55\n",
      "iter = 47 -> delta = 5.89\n",
      "iter = 48 -> delta = 5.3\n",
      "iter = 49 -> delta = 4.77\n",
      "Optimal Value\n",
      "+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n",
      "| 8327 | 8249 | 7244 | 6362 | 5592 | 4972 | ###  | ###  | 2236 | 2235 | 2218 | 2169 | 1943 | 1721 | 1531 |\n",
      "| 8249 | 8194 | 7220 | 6359 | 5591 | 4972 | ###  | ###  | 2519 | 2511 | 2478 | 2215 | 1963 | 1726 | 1533 |\n",
      "| 7244 | 7220 | 7122 | 6319 | 5583 | 4968 | ###  | ###  | 2870 | 2852 | 2535 | 2241 | 1968 | 1728 | 1535 |\n",
      "| 6362 | 6359 | 6319 | 6195 | 5531 | 4931 | ###  | ###  | 3286 | 2901 | 2558 | 2246 | 1970 | 1729 | 1539 |\n",
      "| 5587 | 5586 | 5579 | 5529 | 5393 | 4852 | 4334 | 3787 | 3318 | 2919 | 2561 | 2247 | 1971 | 1734 | 1547 |\n",
      "| 4907 | 4906 | 4904 | 4893 | 4837 | 4698 | 4244 | 3794 | 3328 | 2920 | 2562 | 2248 | 1977 | 1745 | 1549 |\n",
      "| 4308 | 4308 | 4308 | 4304 | 4290 | 4231 | 4094 | 3711 | 3322 | 2918 | 2562 | 2255 | 1991 | 1746 | 1549 |\n",
      "| 3782 | 3782 | 3782 | 3781 | 3776 | 3760 | 3700 | 3569 | 3245 | 2908 | 2559 | 2263 | ###  | ###  | ###  |\n",
      "| 3320 | 3320 | 3320 | 3319 | 3318 | 3313 | 3295 | 3235 | 3112 | 2837 | 2545 | 2258 | ###  | ###  | ###  |\n",
      "| 2913 | 2913 | 2913 | 2913 | 2913 | 2912 | 2906 | 2886 | 2828 | 2714 | 2479 | 2233 | 1983 | 1738 | 1542 |\n",
      "| 2556 | 2556 | 2556 | 2557 | 2565 | 2574 | 2572 | 2555 | 2528 | 2472 | 2367 | 2167 | 1953 | 1736 | 1541 |\n",
      "| 2242 | 2242 | 2243 | 2250 | 2265 | ###  | ###  | 2262 | 2239 | 2213 | 2160 | 2065 | 1893 | 1708 | 1536 |\n",
      "| 1966 | 1967 | 1973 | 1985 | 1986 | ###  | ###  | 1983 | 1982 | 1962 | 1936 | 1887 | 1801 | 1654 | 1509 |\n",
      "| 1726 | 1731 | 1740 | 1741 | 1742 | ###  | ###  | 1739 | 1739 | 1736 | 1719 | 1695 | 1649 | 1572 | 1458 |\n",
      "| 1536 | 1543 | 1545 | 1545 | 1545 | ###  | ###  | 1543 | 1543 | 1542 | 1538 | 1522 | 1497 | 1453 | 1392 |\n",
      "+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n",
      "\n",
      "Optimal Policy\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "| • | ← | ← | ← | ← | ← | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← | ← | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ↙ | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↗ | ▮ | ▮ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "base_environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = -0.01, stopActionPrice = -0.01\n",
    "                               ,goalReward = 1000, punish=-1, j_limit = 15, i_limit = 15, p = 0.8, container=None)\n",
    "\n",
    "theta = {\"max_iter\": 50, \"delta_treshold\": 3}\n",
    "agent = Agent(id=0, environment=base_environment, discount=0.9, theta=theta)\n",
    "\n",
    "agent.value_iteration()\n",
    "\n",
    "print(\"Optimal Value\")\n",
    "agent.print_value()\n",
    "\n",
    "print(\"\\nOptimal Policy\")\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0 -> delta = 850.0\n",
      "iter = 1 -> delta = 762.25\n",
      "iter = 2 -> delta = 677.76\n",
      "iter = 3 -> delta = 609.21\n",
      "iter = 4 -> delta = 547.44\n",
      "iter = 5 -> delta = 492.54\n",
      "iter = 6 -> delta = 443.17\n",
      "iter = 7 -> delta = 398.82\n",
      "iter = 8 -> delta = 358.92\n",
      "iter = 9 -> delta = 323.02\n",
      "iter = 10 -> delta = 290.72\n",
      "iter = 11 -> delta = 261.64\n",
      "iter = 12 -> delta = 235.48\n",
      "iter = 13 -> delta = 211.93\n",
      "iter = 14 -> delta = 190.74\n",
      "iter = 15 -> delta = 171.66\n",
      "iter = 16 -> delta = 154.5\n",
      "iter = 17 -> delta = 139.05\n",
      "iter = 18 -> delta = 125.14\n",
      "iter = 19 -> delta = 112.63\n",
      "iter = 20 -> delta = 101.37\n",
      "iter = 21 -> delta = 91.23\n",
      "iter = 22 -> delta = 82.11\n",
      "iter = 23 -> delta = 73.9\n",
      "iter = 24 -> delta = 66.51\n",
      "iter = 25 -> delta = 59.86\n",
      "iter = 26 -> delta = 53.87\n",
      "iter = 27 -> delta = 48.48\n",
      "iter = 28 -> delta = 43.63\n",
      "iter = 29 -> delta = 39.27\n",
      "iter = 30 -> delta = 35.34\n",
      "iter = 31 -> delta = 31.81\n",
      "iter = 32 -> delta = 28.63\n",
      "iter = 33 -> delta = 25.77\n",
      "iter = 34 -> delta = 23.19\n",
      "iter = 35 -> delta = 20.87\n",
      "iter = 36 -> delta = 18.78\n",
      "iter = 37 -> delta = 16.9\n",
      "iter = 38 -> delta = 15.21\n",
      "iter = 39 -> delta = 13.69\n",
      "iter = 40 -> delta = 12.32\n",
      "iter = 41 -> delta = 11.09\n",
      "iter = 42 -> delta = 9.98\n",
      "iter = 43 -> delta = 8.98\n",
      "iter = 44 -> delta = 8.09\n",
      "iter = 45 -> delta = 7.28\n",
      "iter = 46 -> delta = 6.55\n",
      "iter = 47 -> delta = 5.89\n",
      "iter = 48 -> delta = 5.3\n",
      "iter = 49 -> delta = 4.77\n",
      "Optimal Value\n",
      "+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n",
      "| 8327 | 8249 | 7244 | 6362 | 5592 | 4972 | ###  | ###  | 2237 | 2235 | 2218 | 2170 | 1943 | 1721 | 1531 |\n",
      "| 8249 | 8195 | 7221 | 6359 | 5591 | 4972 | ###  | ###  | 2519 | 2511 | 2478 | 2216 | 1963 | 1726 | 1533 |\n",
      "| 7244 | 7221 | 7122 | 6319 | 5583 | 4968 | ###  | ###  | 2870 | 2852 | 2535 | 2241 | 1968 | 1728 | 1535 |\n",
      "| 6362 | 6359 | 6319 | 6195 | 5532 | 4931 | ###  | ###  | 3286 | 2901 | 2559 | 2246 | 1970 | 1730 | 1539 |\n",
      "| 5587 | 5586 | 5579 | 5529 | 5393 | 4852 | 4334 | 3787 | 3318 | 2919 | 2561 | 2247 | 1971 | 1735 | 1547 |\n",
      "| 4907 | 4906 | 4904 | 4894 | 4837 | 4698 | 4244 | 3795 | 3328 | 2920 | 2562 | 2248 | 1977 | 1745 | 1549 |\n",
      "| 4308 | 4308 | 4308 | 4304 | 4290 | 4231 | 4094 | 3711 | 3322 | 2918 | 2562 | 2255 | 1991 | 1746 | 1549 |\n",
      "| 3782 | 3782 | 3782 | 3781 | 3777 | 3760 | 3700 | 3569 | 3245 | 2908 | 2559 | 2263 | ###  | ###  | ###  |\n",
      "| 3320 | 3320 | 3320 | 3319 | 3318 | 3313 | 3295 | 3235 | 3112 | 2837 | 2545 | 2258 | ###  | ###  | ###  |\n",
      "| 2913 | 2913 | 2913 | 2913 | 2913 | 2912 | 2906 | 2886 | 2828 | 2714 | 2479 | 2233 | 1983 | 1738 | 1542 |\n",
      "| 2556 | 2556 | 2556 | 2557 | 2565 | 2574 | 2572 | 2555 | 2528 | 2472 | 2367 | 2167 | 1953 | 1736 | 1541 |\n",
      "| 2242 | 2242 | 2243 | 2250 | 2265 | ###  | ###  | 2262 | 2239 | 2213 | 2160 | 2065 | 1893 | 1708 | 1536 |\n",
      "| 1966 | 1967 | 1973 | 1985 | 1986 | ###  | ###  | 1983 | 1982 | 1962 | 1936 | 1887 | 1801 | 1654 | 1509 |\n",
      "| 1726 | 1731 | 1740 | 1741 | 1742 | ###  | ###  | 1739 | 1739 | 1736 | 1719 | 1695 | 1649 | 1572 | 1458 |\n",
      "| 1536 | 1543 | 1545 | 1545 | 1546 | ###  | ###  | 1543 | 1543 | 1542 | 1538 | 1522 | 1497 | 1454 | 1392 |\n",
      "+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n",
      "\n",
      "Optimal Policy\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "| • | ← | ← | ← | ← | ← | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← | ← | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ↙ | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↗ | ▮ | ▮ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "without_friction_environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = 0\n",
    "                                           ,stopActionPrice = 0, goalReward = 1000 , punish=-0.01\n",
    "                                           , j_limit = 15, i_limit = 15, p = 0.8, container=None)\n",
    "\n",
    "theta = {\"max_iter\": 50, \"delta_treshold\": 3}\n",
    "agent = Agent(id=0, environment=without_friction_environment, discount=0.9, theta=theta)\n",
    "\n",
    "agent.value_iteration()\n",
    "\n",
    "print(\"Optimal Value\")\n",
    "agent.print_value()\n",
    "\n",
    "print(\"\\nOptimal Policy\")\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0 -> delta = 84.2\n",
      "iter = 1 -> delta = 75.48\n",
      "iter = 2 -> delta = 67.11\n",
      "iter = 3 -> delta = 60.31\n",
      "iter = 4 -> delta = 54.2\n",
      "iter = 5 -> delta = 48.76\n",
      "iter = 6 -> delta = 43.87\n",
      "iter = 7 -> delta = 39.48\n",
      "iter = 8 -> delta = 35.53\n",
      "iter = 9 -> delta = 31.98\n",
      "iter = 10 -> delta = 28.78\n",
      "iter = 11 -> delta = 25.9\n",
      "iter = 12 -> delta = 23.31\n",
      "iter = 13 -> delta = 20.98\n",
      "iter = 14 -> delta = 18.88\n",
      "iter = 15 -> delta = 16.99\n",
      "iter = 16 -> delta = 15.3\n",
      "iter = 17 -> delta = 13.77\n",
      "iter = 18 -> delta = 12.39\n",
      "iter = 19 -> delta = 11.15\n",
      "iter = 20 -> delta = 10.04\n",
      "iter = 21 -> delta = 9.03\n",
      "iter = 22 -> delta = 8.13\n",
      "iter = 23 -> delta = 7.32\n",
      "iter = 24 -> delta = 6.58\n",
      "iter = 25 -> delta = 5.93\n",
      "iter = 26 -> delta = 5.33\n",
      "iter = 27 -> delta = 4.8\n",
      "iter = 28 -> delta = 4.32\n",
      "iter = 29 -> delta = 3.89\n",
      "iter = 30 -> delta = 3.5\n",
      "iter = 31 -> delta = 3.15\n",
      "iter = 32 -> delta = 2.83\n",
      "Value convergenced\n",
      "Optimal Value\n",
      "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "| 803 | 795 | 694 | 606 | 529 | 466 | ### | ### | 192 | 192 | 191 | 186 | 163 | 141 | 122 |\n",
      "| 795 | 789 | 692 | 605 | 528 | 466 | ### | ### | 221 | 220 | 217 | 190 | 165 | 141 | 122 |\n",
      "| 694 | 692 | 682 | 601 | 528 | 466 | ### | ### | 256 | 254 | 222 | 193 | 166 | 141 | 122 |\n",
      "| 606 | 605 | 601 | 589 | 522 | 462 | ### | ### | 298 | 259 | 225 | 193 | 166 | 142 | 123 |\n",
      "| 528 | 528 | 527 | 522 | 509 | 454 | 402 | 348 | 301 | 261 | 225 | 193 | 166 | 142 | 123 |\n",
      "| 460 | 460 | 460 | 459 | 453 | 439 | 393 | 348 | 302 | 261 | 225 | 194 | 166 | 143 | 123 |\n",
      "| 400 | 400 | 400 | 400 | 398 | 392 | 379 | 340 | 301 | 261 | 225 | 194 | 168 | 143 | 124 |\n",
      "| 347 | 347 | 347 | 347 | 347 | 345 | 339 | 326 | 293 | 260 | 225 | 195 | ### | ### | ### |\n",
      "| 301 | 301 | 301 | 301 | 301 | 300 | 298 | 292 | 280 | 253 | 223 | 195 | ### | ### | ### |\n",
      "| 260 | 260 | 260 | 260 | 260 | 260 | 259 | 257 | 252 | 240 | 217 | 192 | 167 | 142 | 123 |\n",
      "| 224 | 224 | 224 | 224 | 225 | 226 | 226 | 224 | 222 | 216 | 205 | 185 | 164 | 142 | 123 |\n",
      "| 193 | 193 | 193 | 194 | 195 | ### | ### | 195 | 193 | 190 | 185 | 175 | 158 | 139 | 122 |\n",
      "| 165 | 165 | 166 | 167 | 167 | ### | ### | 167 | 167 | 165 | 162 | 157 | 149 | 134 | 119 |\n",
      "| 141 | 142 | 143 | 143 | 143 | ### | ### | 143 | 143 | 142 | 141 | 138 | 134 | 126 | 114 |\n",
      "| 122 | 123 | 123 | 123 | 123 | ### | ### | 123 | 123 | 123 | 122 | 121 | 118 | 114 | 108 |\n",
      "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "\n",
      "Optimal Policy\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "| • | ← | ← | ← | ← | ← | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← | ← | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ↙ | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ▮ | ▮ | ▮ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↗ | ▮ | ▮ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ▮ | ▮ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "with_extreme_friction_environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = -1\n",
    "                                               ,stopActionPrice = -0.8 ,goalReward = 100 , punish=-10\n",
    "                                               ,j_limit = 15, i_limit = 15, p = 0.8, container=None)\n",
    "\n",
    "theta = {\"max_iter\": 50, \"delta_treshold\": 3}\n",
    "agent = Agent(id=0, environment=with_extreme_friction_environment, discount=0.9, theta=theta)\n",
    "\n",
    "agent.value_iteration()\n",
    "\n",
    "print(\"Optimal Value\")\n",
    "agent.print_value()\n",
    "\n",
    "print(\"\\nOptimal Policy\")\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "env.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "83fad98a7911d3a2a55c2e5234aea09e74d0252d0d10d90172c6e09f21426bdf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
