{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/zhpinkman/armed-bandit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ./armed-bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impor Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mBMoPLmGbrIn"
   },
   "outputs": [],
   "source": [
    "from amalearn.reward import RewardBase\n",
    "from amalearn.agent import AgentBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YBACGmh0brIr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from amalearn.environment import EnvironmentBase\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "id": "pH6sNHxPbrIs"
   },
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "# Action:\n",
    "# 0 1 2\n",
    "# 3 4 5\n",
    "# 6 7 8\n",
    "\n",
    "class Environment(EnvironmentBase):\n",
    "    def __init__(self, obstacle = [] ,id = 0, action_count=9, actionPrice = -1, goalReward = 100\n",
    "                 , punish=-10, j_limit = 10, i_limit = 10, p = 0.8, container=None):\n",
    "        \"\"\"\n",
    "        initialize your variables\n",
    "        \"\"\"\n",
    "        \n",
    "        self.obstacle = obstacle\n",
    "        \n",
    "        self.x_min = 1\n",
    "        self.x_max = i_limit\n",
    "        \n",
    "        self.y_min = 1\n",
    "        self.y_max = j_limit\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "        self.action_count = action_count\n",
    "        self.actionPrice = actionPrice\n",
    "        self.goalReward = goalReward\n",
    "        self.punish = punish\n",
    "        self.p = p\n",
    "        \n",
    "        action_space = Discrete(action_count)\n",
    "        state_space = Box(low=1, high=max(i_limit, j_limit), shape=(1,2), dtype=int)\n",
    "        \n",
    "        self.action_list = list(range(1,10))\n",
    "        self.state_list = []\n",
    "        \n",
    "        for i in range(1, i_limit+1):\n",
    "            for j in range(1, j_limit+1):\n",
    "                self.state_list.append(np.array([i, j]))\n",
    "        \n",
    "        super(Environment, self).__init__(action_space=action_space, state_space=state_space, id=id ,container=container)\n",
    "\n",
    "        \n",
    "    def isStatePossible(self, state):\n",
    "        \"\"\"if given state is possible (not out of the grid and not obstacle) return ture\"\"\"\n",
    "        if self.x_min <= state[0] <= self.x_max and self.y_min <= state[1] <= self.y_max:\n",
    "            for obstacle_item in self.obstacle:\n",
    "                if (state==obstacle_item).all():\n",
    "                    return False\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def isAccessible(self, state, state_p):\n",
    "        \"\"\"if given state is Accesible (we can reach state_p by doing an action from state) return true\"\"\"\n",
    "        return abs(state[0]-state_p[0]) <= 1 and abs(state[1] - state_p[1]) <= 1 and self.isStatePossible(state_p)\n",
    "            \n",
    "    def getTransitionStatesAndProbs(self, state, action, state_p):\n",
    "        \"\"\"return probability of transition or T(sp,a,s)\"\"\"\n",
    "        \n",
    "        actions = self.available_actions_state(state)\n",
    "        \n",
    "        if (self.calculate_next_state(state,action)==state_p).all():\n",
    "            if self.isAccessible(state, state_p):\n",
    "                return self.p + (1-self.p) / len(actions)\n",
    "            else:\n",
    "                return self.p\n",
    "        else:\n",
    "            if self.isAccessible(state, state_p):\n",
    "                return (1-self.p) / len(actions)\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "    \n",
    "    def getReward(self, state, action, state_p):\n",
    "        \"\"\"return reward of transition\"\"\"\n",
    "        if self.terminated_state(state_p):\n",
    "            return self.actionPrice + self.goalReward\n",
    "        \n",
    "        if self.isStatePossible(state_p):\n",
    "            return self.actionPrice\n",
    "        else:\n",
    "            return self.actionPrice + self.punish\n",
    "        \n",
    "    def sample_all_rewards(self):\n",
    "        return \n",
    "    \n",
    "    def calculate_reward(self, action):\n",
    "        return self.getReward(self.current_state, action, self.calculate_next_state(self.current_state, action))\n",
    "\n",
    "    def available_states_state(self, state):\n",
    "        states = []\n",
    "        for i in [-1, 0, +1]:\n",
    "            for j in [-1, 0, +1]:\n",
    "                new_state = np.array([state[0]+i, state[1]+j])\n",
    "                \n",
    "                if self.isAccessible(state, new_state):\n",
    "                    states.append(new_state)\n",
    "        return states\n",
    "    \n",
    "    def terminated(self):\n",
    "        return self.terminated_state(self.current_state)\n",
    "    \n",
    "    def terminated_state(self, state):\n",
    "        return (state==np.array([1,1])).all()\n",
    "        \n",
    "    def observe(self):\n",
    "        return self.current_state \n",
    "\n",
    "    def available_actions(self):\n",
    "        return self.available_actions_state(self.current_state)\n",
    "    \n",
    "    def available_actions_state(self, state):\n",
    "        output_actions = []\n",
    "        for action in range(self.action_count):\n",
    "            next_state = self.calculate_next_state(state, action)\n",
    "            \n",
    "            if self.isAccessible(state, next_state):\n",
    "                output_actions.append(action)\n",
    "        \n",
    "        return output_actions\n",
    "        \n",
    "    \n",
    "    def calculate_next_state(self, state, action):\n",
    "        return np.array([state[0] + (action%3 -1), state[1] + (int(action/3)-1) ])\n",
    "        \n",
    "    def next_state(self, action):\n",
    "        actions = self.available_actions()\n",
    "        \n",
    "        if action not in actions:\n",
    "            actions.append(action)\n",
    "                \n",
    "        probabilities = []\n",
    "                \n",
    "        for action2 in actions:\n",
    "            state2 = self.calculate_next_state(self.current_state, action2)\n",
    "            probabilities.append(self.getTransitionStatesAndProbs(self.current_state, action, state2))\n",
    "        \n",
    "        final_action = random.choices(population=actions, weights=probabilities, k=1)[0]\n",
    "        \n",
    "        real_next_state = self.calculate_next_state(self.current_state, final_action)\n",
    "        \n",
    "        if not self.isStatePossible(real_next_state):\n",
    "            real_next_state = self.current_state\n",
    "        \n",
    "        self.last_action = action\n",
    "        \n",
    "        self.sliped = not (final_action==action)\n",
    "        \n",
    "        self.current_state = real_next_state\n",
    "        \n",
    "        return\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = np.array([15, 15])\n",
    "        \n",
    "        self.last_action = None\n",
    "        self.sliped = None\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"{self.current_state} \\t {self.last_action} \\t {self.sliped}\")\n",
    "        return \n",
    "\n",
    "    def close(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_states =  [np.array([7, 1]), np.array([8, 1]), np.array([7, 2]), np.array([8, 2])\n",
    "                ,np.array([7, 3]), np.array([8, 3]), np.array([7, 4]), np.array([8, 4])\n",
    "                ,np.array([13, 8]), np.array([14, 8]), np.array([15, 8])\n",
    "                ,np.array([13, 9]), np.array([14, 9]), np.array([15, 9])\n",
    "                ,np.array([6, 12]), np.array([7, 12]), np.array([6, 13]), np.array([7, 13])\n",
    "                ,np.array([6, 14]), np.array([7, 14]), np.array([6, 15]), np.array([7, 15])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = -1, goalReward = 100\n",
    "                                , punish=-10, j_limit = 15, i_limit = 15, p = 0.8, container=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "id": "898Jlhsycyes"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Agent(AgentBase):\n",
    "    def __init__(self, id, environment, discount, theta):\n",
    "        \n",
    "        # initialize a random policy and V(s) = 0 for each state\n",
    "        self.environment = environment\n",
    "        \n",
    "        # mapp states to its ids\n",
    "#         self.mapp = {}\n",
    "        \n",
    "        # init V\n",
    "        self.V = {}\n",
    "        \n",
    "        # init policy\n",
    "        self.policy = {}\n",
    "        \n",
    "        super(Agent, self).__init__(id, environment)\n",
    "        \n",
    "        self.discount = discount\n",
    "        \n",
    "        self.theta = theta\n",
    "                \n",
    "        self.value_initialization()\n",
    "        \n",
    "        self.policy_initialization()\n",
    "    \n",
    "    def value_initialization(self):\n",
    "        for state in self.environment.state_list:\n",
    "            self.V[tuple(state)] = 0\n",
    "        \n",
    "    def policy_initialization(self):\n",
    "        for state in self.environment.state_list:\n",
    "            self.policy[tuple(state)] = random.choice(self.environment.action_list)\n",
    "        \n",
    "    def policy_evaluation(self):\n",
    "        pass\n",
    "    \n",
    "    def policy_improvement(self):\n",
    "        pass\n",
    "    \n",
    "    def value_iteration(self):\n",
    "        for iter in range(self.theta[\"max_iter\"]):\n",
    "            new_V = {}\n",
    "\n",
    "            delta = 0\n",
    "            for state in self.environment.state_list:\n",
    "                new_V[tuple(state)] = -math.inf\n",
    "\n",
    "                available_actions = self.environment.available_actions_state(state)\n",
    "                available_states  = self.environment.available_states_state(state)\n",
    "\n",
    "                for action in available_actions:\n",
    "                    sum = 0\n",
    "                    for state_p in available_states:\n",
    "                        p_sp = self.environment.getTransitionStatesAndProbs(state, action, state_p)\n",
    "                        r_sp = self.environment.getReward(state, action, state_p)\n",
    "                        v_sp = self.V[tuple(state_p)]\n",
    "\n",
    "                        sum += p_sp * (r_sp + self.discount * v_sp)\n",
    "\n",
    "                    new_V[tuple(state)] = max(new_V[tuple(state)], sum)\n",
    "                delta = max(delta, abs(self.V[tuple(state)] - new_V[tuple(state)]))\n",
    "                \n",
    "            print(f\"iter = {iter} -> delta = {round(delta, 2)}\")\n",
    "            self.V = deepcopy(new_V)\n",
    "\n",
    "            if delta < self.theta[\"delta_treshold\"]:\n",
    "                break\n",
    "    \n",
    "    def policy_extraction(self):\n",
    "        for state in self.environment.state_list:\n",
    "\n",
    "            available_actions = self.environment.available_actions_state(state)\n",
    "            available_states  = self.environment.available_states_state(state)\n",
    "            \n",
    "            max_value = -math.inf\n",
    "            argmax = None\n",
    "            \n",
    "            for action in available_actions:\n",
    "                sum = 0\n",
    "                for state_p in available_states:\n",
    "                    p_sp = self.environment.getTransitionStatesAndProbs(state, action, state_p)\n",
    "                    v_sp = self.V[tuple(state_p)]\n",
    "\n",
    "                    sum += p_sp * v_sp\n",
    "            \n",
    "                if (state==np.array([1,1])).all():\n",
    "                    print(action, sum)\n",
    "            \n",
    "                if sum > max_value:\n",
    "                    max_value = sum\n",
    "                    argmax = action\n",
    "                    \n",
    "            self.policy[tuple(state)] = action\n",
    "    \n",
    "    def print_value(self):\n",
    "        p = PrettyTable()\n",
    "\n",
    "        for j in range(1,16):\n",
    "            row = []\n",
    "            for i in range(1, 16):\n",
    "                row.append(int(self.V[(i,j)]))\n",
    "            p.add_row(row)\n",
    "\n",
    "        print (p.get_string(header=False, border=True))\n",
    "    \n",
    "    def print_policy(self):\n",
    "        p = PrettyTable()\n",
    "\n",
    "        for j in range(1,16):\n",
    "            row = []\n",
    "            for i in range(1, 16):\n",
    "                row.append(int(self.policy[(i,j)]))\n",
    "            p.add_row(row)\n",
    "\n",
    "        print (p.get_string(header=False, border=True))\n",
    "    \n",
    "    def take_action(self) -> (object, float, bool, object):\n",
    "        # in this method, you MUST call the `step` method of \n",
    "        # the environment and observe the results and return them like:\n",
    "        # return observation, reward, done, info\n",
    "\n",
    "        return self.environment.step(random.choice(self.environment.action_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = {\"max_iter\": 5, \"delta_treshold\": 5}\n",
    "agent = Agent(id=0, environment=base_environment, discount=0.9, theta=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0 -> delta = 84.0\n",
      "iter = 1 -> delta = 75.33\n"
     ]
    }
   ],
   "source": [
    "agent.value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.print_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.print_privacy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "| 783 | 776 | 675 | 587 | 510 | 448 | 398 | 174 | 174 | 174 | 173 | 168 | 145 | 123 | 104 |\n",
      "| 776 | 770 | 673 | 587 | 510 | 448 | 398 | 203 | 203 | 202 | 199 | 172 | 147 | 123 | 104 |\n",
      "| 675 | 673 | 663 | 583 | 509 | 448 | 398 | 239 | 238 | 236 | 204 | 175 | 148 | 124 | 104 |\n",
      "| 587 | 587 | 583 | 570 | 504 | 444 | 390 | 326 | 279 | 241 | 207 | 175 | 148 | 124 | 105 |\n",
      "| 509 | 509 | 509 | 504 | 490 | 436 | 384 | 329 | 283 | 243 | 207 | 175 | 148 | 124 | 105 |\n",
      "| 441 | 441 | 441 | 440 | 434 | 421 | 375 | 330 | 284 | 243 | 207 | 176 | 148 | 125 | 106 |\n",
      "| 382 | 382 | 382 | 381 | 380 | 374 | 360 | 322 | 283 | 243 | 207 | 176 | 150 | 125 | 106 |\n",
      "| 329 | 329 | 329 | 329 | 328 | 327 | 321 | 308 | 275 | 242 | 207 | 177 | 151 | 125 | 106 |\n",
      "| 283 | 283 | 283 | 283 | 283 | 282 | 280 | 274 | 262 | 234 | 205 | 177 | 151 | 125 | 105 |\n",
      "| 242 | 242 | 242 | 242 | 242 | 242 | 241 | 239 | 234 | 222 | 199 | 174 | 149 | 125 | 105 |\n",
      "| 206 | 206 | 206 | 206 | 207 | 208 | 208 | 206 | 203 | 198 | 187 | 167 | 146 | 124 | 105 |\n",
      "| 175 | 175 | 175 | 176 | 177 | 179 | 179 | 177 | 175 | 172 | 167 | 157 | 140 | 122 | 104 |\n",
      "| 147 | 147 | 148 | 149 | 149 | 149 | 149 | 149 | 149 | 147 | 144 | 139 | 131 | 116 | 102 |\n",
      "| 123 | 124 | 125 | 125 | 125 | 125 | 125 | 125 | 125 | 124 | 123 | 120 | 116 | 108 |  97 |\n",
      "| 104 | 105 | 105 | 105 | 105 | 105 | 105 | 105 | 105 | 105 | 105 | 103 | 100 |  96 |  90 |\n",
      "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "p = PrettyTable()\n",
    "\n",
    "for j in range(1,16):\n",
    "    row = []\n",
    "    for i in range(1, 16):\n",
    "        row.append(int(agent.V[(i,j)]))\n",
    "    p.add_row(row)\n",
    "\n",
    "print (p.get_string(header=False, border=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "| 8 | 8 | 8 | 8 | 8 | 7 | 6 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 7 |\n",
      "| 8 | 8 | 8 | 8 | 8 | 7 | 6 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 7 |\n",
      "| 8 | 8 | 8 | 8 | 8 | 7 | 6 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 7 |\n",
      "| 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 7 |\n",
      "| 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 7 |\n",
      "| 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 7 |\n",
      "| 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 7 | 6 | 5 | 4 |\n",
      "| 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 7 | 6 | 2 | 1 |\n",
      "| 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 7 |\n",
      "| 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 7 |\n",
      "| 8 | 8 | 8 | 8 | 7 | 6 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 7 |\n",
      "| 8 | 8 | 8 | 8 | 7 | 6 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 7 |\n",
      "| 8 | 8 | 8 | 8 | 7 | 6 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 7 |\n",
      "| 8 | 8 | 8 | 8 | 7 | 6 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 8 | 7 |\n",
      "| 5 | 5 | 5 | 5 | 4 | 3 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 4 |\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "p = PrettyTable()\n",
    "\n",
    "for j in range(1,16):\n",
    "    row = []\n",
    "    for i in range(1, 16):\n",
    "        row.append(int(agent.policy[(i,j)]))\n",
    "    p.add_row(row)\n",
    "\n",
    "print (p.get_string(header=False, border=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "env.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "83fad98a7911d3a2a55c2e5234aea09e74d0252d0d10d90172c6e09f21426bdf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
