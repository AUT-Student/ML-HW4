{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/zhpinkman/armed-bandit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ./armed-bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impor Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mBMoPLmGbrIn"
   },
   "outputs": [],
   "source": [
    "from amalearn.reward import RewardBase\n",
    "from amalearn.agent import AgentBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YBACGmh0brIr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from amalearn.environment import EnvironmentBase\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "id": "pH6sNHxPbrIs"
   },
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "# Action:\n",
    "# 0 1 2\n",
    "# 3 4 5\n",
    "# 6 7 8\n",
    "\n",
    "class Environment(EnvironmentBase):\n",
    "    def __init__(self, obstacle = [] ,id = 0, action_count=9, actionPrice = -1, goalReward = 100\n",
    "                 , punish=-10, j_limit = 10, i_limit = 10, p = 0.8, container=None):\n",
    "        \"\"\"\n",
    "        initialize your variables\n",
    "        \"\"\"\n",
    "        \n",
    "        self.obstacle = obstacle\n",
    "        \n",
    "        self.x_min = 1\n",
    "        self.x_max = i_limit\n",
    "        \n",
    "        self.y_min = 1\n",
    "        self.y_max = j_limit\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "        self.action_count = action_count\n",
    "        self.actionPrice = actionPrice\n",
    "        self.goalReward = goalReward\n",
    "        self.punish = punish\n",
    "        self.p = p\n",
    "        \n",
    "        action_space = Discrete(action_count)\n",
    "        state_space = Box(low=1, high=max(i_limit, j_limit), shape=(1,2), dtype=int)\n",
    "        \n",
    "        self.action_list = list(range(1,10))\n",
    "        self.state_list = []\n",
    "        \n",
    "        for i in range(1, i_limit+1):\n",
    "            for j in range(1, j_limit+1):\n",
    "                self.state_list.append(np.array([i, j]))\n",
    "        \n",
    "        super(Environment, self).__init__(action_space=action_space, state_space=state_space, id=id ,container=container)\n",
    "\n",
    "        \n",
    "    def isStatePossible(self, state):\n",
    "        \"\"\"if given state is possible (not out of the grid and not obstacle) return ture\"\"\"\n",
    "        if self.x_min <= state[0] <= self.x_max and self.y_min <= state[1] <= self.y_max:\n",
    "            for obstacle_item in self.obstacle:\n",
    "                if (state==obstacle_item).all():\n",
    "                    return False\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def isAccessible(self, state, state_p):\n",
    "        \"\"\"if given state is Accesible (we can reach state_p by doing an action from state) return true\"\"\"\n",
    "        return abs(state[0]-state_p[0]) <= 1 and abs(state[1] - state_p[1]) <= 1 and self.isStatePossible(state_p)\n",
    "            \n",
    "    def getTransitionStatesAndProbs(self, state, action, state_p):\n",
    "        \"\"\"return probability of transition or T(sp,a,s)\"\"\"\n",
    "        \n",
    "        actions = self.available_actions_state(state)\n",
    "        \n",
    "        if (self.calculate_next_state(state,action)==state_p).all():\n",
    "            if self.isAccessible(state, state_p):\n",
    "                return self.p + (1-self.p) / len(actions)\n",
    "            else:\n",
    "                return self.p\n",
    "        else:\n",
    "            if self.isAccessible(state, state_p):\n",
    "                return (1-self.p) / len(actions)\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "    \n",
    "    def getReward(self, state, action, state_p):\n",
    "        \"\"\"return reward of transition\"\"\"\n",
    "        if self.terminated_state(state_p):\n",
    "            return self.actionPrice + self.goalReward\n",
    "        \n",
    "        if self.isStatePossible(state_p):\n",
    "            return self.actionPrice\n",
    "        else:\n",
    "            return self.actionPrice + self.punish\n",
    "        \n",
    "    def sample_all_rewards(self):\n",
    "        return \n",
    "    \n",
    "    def calculate_reward(self, action):\n",
    "        return self.getReward(self.current_state, action, self.calculate_next_state(self.current_state, action))\n",
    "\n",
    "    def available_states_state(self, state):\n",
    "        states = []\n",
    "        for i in [-1, 0, +1]:\n",
    "            for j in [-1, 0, +1]:\n",
    "                new_state = np.array([state[0]+i, state[1]+j])\n",
    "                \n",
    "                if self.isAccessible(state, new_state):\n",
    "                    states.append(new_state)\n",
    "        return states\n",
    "    \n",
    "    def terminated(self):\n",
    "        return self.terminated_state(self.current_state)\n",
    "    \n",
    "    def terminated_state(self, state):\n",
    "        return (state==np.array([1,1])).all()\n",
    "        \n",
    "    def observe(self):\n",
    "        return self.current_state \n",
    "\n",
    "    def available_actions(self):\n",
    "        return self.available_actions_state(self.current_state)\n",
    "    \n",
    "    def available_actions_state(self, state):\n",
    "        output_actions = []\n",
    "        for action in range(self.action_count):\n",
    "            next_state = self.calculate_next_state(state, action)\n",
    "            \n",
    "            if self.isAccessible(state, next_state):\n",
    "                output_actions.append(action)\n",
    "        \n",
    "        return output_actions\n",
    "        \n",
    "    \n",
    "    def calculate_next_state(self, state, action):\n",
    "        return np.array([state[0] + (action%3 -1), state[1] + (int(action/3)-1) ])\n",
    "        \n",
    "    def next_state(self, action):\n",
    "        actions = self.available_actions()\n",
    "        \n",
    "        if action not in actions:\n",
    "            actions.append(action)\n",
    "                \n",
    "        probabilities = []\n",
    "                \n",
    "        for action2 in actions:\n",
    "            state2 = self.calculate_next_state(self.current_state, action2)\n",
    "            probabilities.append(self.getTransitionStatesAndProbs(self.current_state, action, state2))\n",
    "        \n",
    "        final_action = random.choices(population=actions, weights=probabilities, k=1)[0]\n",
    "        \n",
    "        real_next_state = self.calculate_next_state(self.current_state, final_action)\n",
    "        \n",
    "        if not self.isStatePossible(real_next_state):\n",
    "            real_next_state = self.current_state\n",
    "        \n",
    "        self.last_action = action\n",
    "        \n",
    "        self.sliped = not (final_action==action)\n",
    "        \n",
    "        self.current_state = real_next_state\n",
    "        \n",
    "        return\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = np.array([15, 15])\n",
    "        \n",
    "        self.last_action = None\n",
    "        self.sliped = None\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"{self.current_state} \\t {self.last_action} \\t {self.sliped}\")\n",
    "        return \n",
    "\n",
    "    def close(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_states =  [np.array([7, 1]), np.array([8, 1]), np.array([7, 2]), np.array([8, 2])\n",
    "                ,np.array([7, 3]), np.array([8, 3]), np.array([7, 4]), np.array([8, 4])\n",
    "                ,np.array([13, 8]), np.array([14, 8]), np.array([15, 8])\n",
    "                ,np.array([13, 9]), np.array([14, 9]), np.array([15, 9])\n",
    "                ,np.array([6, 12]), np.array([7, 12]), np.array([6, 13]), np.array([7, 13])\n",
    "                ,np.array([6, 14]), np.array([7, 14]), np.array([6, 15]), np.array([7, 15])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = -1, goalReward = 100\n",
    "                                , punish=-10, j_limit = 15, i_limit = 15, p = 0.8, container=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "id": "898Jlhsycyes"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Agent(AgentBase):\n",
    "    def __init__(self, id, environment, discount, theta):\n",
    "        \n",
    "        # initialize a random policy and V(s) = 0 for each state\n",
    "        self.environment = environment\n",
    "        \n",
    "        # mapp states to its ids\n",
    "#         self.mapp = {}\n",
    "        \n",
    "        # init V\n",
    "        self.V = {}\n",
    "        \n",
    "        # init policy\n",
    "        self.policy = {}\n",
    "        \n",
    "        super(Agent, self).__init__(id, environment)\n",
    "        \n",
    "        self.discount = discount\n",
    "        \n",
    "        self.theta = theta\n",
    "                \n",
    "        self.value_initialization()\n",
    "        \n",
    "        self.policy_initialization()\n",
    "    \n",
    "    def value_initialization(self):\n",
    "        for state in self.environment.state_list:\n",
    "            self.V[tuple(state)] = 0\n",
    "        \n",
    "    def policy_initialization(self):\n",
    "        for state in self.environment.state_list:\n",
    "            self.policy[tuple(state)] = random.choice(self.environment.action_list)\n",
    "        \n",
    "    def policy_evaluation(self):\n",
    "        pass\n",
    "    \n",
    "    def policy_improvement(self):\n",
    "        pass\n",
    "    \n",
    "    def value_iteration(self):\n",
    "        for iter in range(self.theta[\"max_iter\"]):\n",
    "            new_V = {}\n",
    "\n",
    "            delta = 0\n",
    "            for state in self.environment.state_list:\n",
    "                new_V[tuple(state)] = -math.inf\n",
    "\n",
    "                available_actions = self.environment.available_actions_state(state)\n",
    "                available_states  = self.environment.available_states_state(state)\n",
    "\n",
    "                for action in available_actions:\n",
    "                    sum = 0\n",
    "                    for state_p in available_states:\n",
    "                        p_sp = self.environment.getTransitionStatesAndProbs(state, action, state_p)\n",
    "                        r_sp = self.environment.getReward(state, action, state_p)\n",
    "                        v_sp = self.V[tuple(state_p)]\n",
    "\n",
    "                        sum += p_sp * (r_sp + self.discount * v_sp)\n",
    "\n",
    "                    new_V[tuple(state)] = max(new_V[tuple(state)], sum)\n",
    "                delta = max(delta, abs(self.V[tuple(state)] - new_V[tuple(state)]))\n",
    "                \n",
    "            print(f\"iter = {iter} -> delta = {round(delta, 2)}\")\n",
    "            self.V = deepcopy(new_V)\n",
    "\n",
    "            if delta < self.theta[\"delta_treshold\"]:\n",
    "                break\n",
    "    \n",
    "    def policy_extraction(self):\n",
    "        for state in self.environment.state_list:\n",
    "\n",
    "            available_actions = self.environment.available_actions_state(state)\n",
    "            available_states  = self.environment.available_states_state(state)\n",
    "            \n",
    "            max_value = -math.inf\n",
    "            argmax = None\n",
    "            \n",
    "            for action in available_actions:\n",
    "                sum = 0\n",
    "                for state_p in available_states:\n",
    "                    p_sp = self.environment.getTransitionStatesAndProbs(state, action, state_p)\n",
    "                    v_sp = self.V[tuple(state_p)]\n",
    "\n",
    "                    sum += p_sp * v_sp\n",
    "            \n",
    "                if (state==np.array([1,1])).all():\n",
    "                    print(action, sum)\n",
    "            \n",
    "                if sum > max_value:\n",
    "                    max_value = sum\n",
    "                    argmax = action\n",
    "                    \n",
    "            self.policy[tuple(state)] = argmax\n",
    "    \n",
    "    def print_value(self):\n",
    "        p = PrettyTable()\n",
    "\n",
    "        for j in range(1,16):\n",
    "            row = []\n",
    "            for i in range(1, 16):\n",
    "                row.append(int(self.V[(i,j)]))\n",
    "            p.add_row(row)\n",
    "\n",
    "        print (p.get_string(header=False, border=True))\n",
    "    \n",
    "    def print_policy(self):\n",
    "        p = PrettyTable()\n",
    "\n",
    "        for j in range(1,16):\n",
    "            row = []\n",
    "            for i in range(1, 16):\n",
    "                row.append(self.action_symbol(int(self.policy[(i,j)])))\n",
    "            p.add_row(row)\n",
    "\n",
    "        print (p.get_string(header=False, border=True))\n",
    "    \n",
    "    def take_action(self) -> (object, float, bool, object):\n",
    "        # observation, reward, done, info\n",
    "        return self.environment.step(random.choice(self.environment.action_list))\n",
    "    \n",
    "    def action_symbol(self, action):\n",
    "        if action == 0:\n",
    "            return \"↖\"\n",
    "        elif action == 1:\n",
    "            return \"↑\"\n",
    "        elif action == 2:\n",
    "            return \"↗\"\n",
    "        elif action == 3:\n",
    "            return \"←\"\n",
    "        elif action == 4:\n",
    "            return \"•\"\n",
    "        elif action == 5:\n",
    "            return \"→\"\n",
    "        elif action == 6:\n",
    "            return \"↙\"\n",
    "        elif action == 7:\n",
    "            return \"↓\"\n",
    "        elif action == 8:\n",
    "            return \"↘\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = {\"max_iter\": 50, \"delta_treshold\": 3}\n",
    "agent = Agent(id=0, environment=base_environment, discount=0.9, theta=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0 -> delta = 84.0\n",
      "iter = 1 -> delta = 75.33\n",
      "iter = 2 -> delta = 66.97\n",
      "iter = 3 -> delta = 60.19\n",
      "iter = 4 -> delta = 54.09\n",
      "iter = 5 -> delta = 48.66\n",
      "iter = 6 -> delta = 43.79\n",
      "iter = 7 -> delta = 39.4\n",
      "iter = 8 -> delta = 35.46\n",
      "iter = 9 -> delta = 31.91\n",
      "iter = 10 -> delta = 28.72\n",
      "iter = 11 -> delta = 25.85\n",
      "iter = 12 -> delta = 23.27\n",
      "iter = 13 -> delta = 20.94\n",
      "iter = 14 -> delta = 18.84\n",
      "iter = 15 -> delta = 16.96\n",
      "iter = 16 -> delta = 15.26\n",
      "iter = 17 -> delta = 13.74\n",
      "iter = 18 -> delta = 12.36\n",
      "iter = 19 -> delta = 11.13\n",
      "iter = 20 -> delta = 10.01\n",
      "iter = 21 -> delta = 9.01\n",
      "iter = 22 -> delta = 8.11\n",
      "iter = 23 -> delta = 7.3\n",
      "iter = 24 -> delta = 6.57\n",
      "iter = 25 -> delta = 5.91\n",
      "iter = 26 -> delta = 5.32\n",
      "iter = 27 -> delta = 4.79\n",
      "iter = 28 -> delta = 4.31\n",
      "iter = 29 -> delta = 3.88\n",
      "iter = 30 -> delta = 3.49\n",
      "iter = 31 -> delta = 3.14\n",
      "iter = 32 -> delta = 2.83\n"
     ]
    }
   ],
   "source": [
    "agent.value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "| 801 | 793 | 693 | 605 | 528 | 466 | 415 | 192 | 192 | 192 | 190 | 185 | 163 | 141 | 122 |\n",
      "| 793 | 788 | 690 | 604 | 527 | 466 | 415 | 220 | 220 | 219 | 216 | 190 | 165 | 141 | 122 |\n",
      "| 693 | 690 | 681 | 600 | 527 | 465 | 415 | 257 | 255 | 254 | 222 | 193 | 165 | 141 | 122 |\n",
      "| 605 | 604 | 600 | 588 | 522 | 461 | 408 | 343 | 297 | 258 | 224 | 193 | 165 | 141 | 122 |\n",
      "| 527 | 527 | 526 | 521 | 508 | 454 | 402 | 347 | 300 | 260 | 225 | 193 | 166 | 142 | 123 |\n",
      "| 459 | 459 | 459 | 458 | 452 | 438 | 393 | 348 | 301 | 260 | 225 | 193 | 166 | 143 | 123 |\n",
      "| 399 | 399 | 399 | 399 | 397 | 391 | 378 | 340 | 301 | 260 | 225 | 194 | 167 | 143 | 123 |\n",
      "| 347 | 347 | 347 | 347 | 346 | 344 | 338 | 325 | 293 | 259 | 224 | 195 | 169 | 143 | 123 |\n",
      "| 300 | 300 | 300 | 300 | 300 | 300 | 298 | 292 | 280 | 252 | 223 | 194 | 169 | 142 | 123 |\n",
      "| 260 | 260 | 260 | 260 | 260 | 260 | 259 | 257 | 251 | 240 | 216 | 192 | 167 | 142 | 123 |\n",
      "| 224 | 224 | 224 | 224 | 225 | 226 | 226 | 224 | 221 | 216 | 205 | 185 | 164 | 142 | 123 |\n",
      "| 193 | 193 | 193 | 193 | 195 | 196 | 196 | 195 | 192 | 190 | 184 | 175 | 158 | 139 | 122 |\n",
      "| 165 | 165 | 166 | 167 | 167 | 167 | 167 | 167 | 167 | 165 | 162 | 157 | 148 | 134 | 119 |\n",
      "| 141 | 141 | 142 | 143 | 143 | 143 | 142 | 142 | 142 | 142 | 140 | 138 | 133 | 126 | 114 |\n",
      "| 122 | 123 | 123 | 123 | 123 | 123 | 123 | 123 | 123 | 123 | 122 | 121 | 118 | 114 | 108 |\n",
      "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n"
     ]
    }
   ],
   "source": [
    "agent.print_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 800.12854447486\n",
      "5 793.8866168122071\n",
      "7 793.8865284348302\n",
      "8 789.5489453900622\n"
     ]
    }
   ],
   "source": [
    "agent.policy_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "| • | ← | ← | ← | ← | ← | ← | ↘ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↘ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↘ | ↓ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← | ← | ↙ | ↙ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ↙ | ← | ← |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ↖ | ↖ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↙ | ↙ |\n",
      "| ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ← | ← |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↗ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ↖ | ↗ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ↖ | ↗ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "| ↗ | ↗ | ↗ | ↗ | ↑ | ↖ | ↗ | ↑ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ | ↖ |\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "env.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "83fad98a7911d3a2a55c2e5234aea09e74d0252d0d10d90172c6e09f21426bdf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
