{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/zhpinkman/armed-bandit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ./armed-bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impor Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mBMoPLmGbrIn"
   },
   "outputs": [],
   "source": [
    "from amalearn.reward import RewardBase\n",
    "from amalearn.agent import AgentBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YBACGmh0brIr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from amalearn.environment import EnvironmentBase\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "id": "pH6sNHxPbrIs"
   },
   "outputs": [],
   "source": [
    "# Action:\n",
    "# 0 1 2\n",
    "# 3 4 5\n",
    "# 6 7 8\n",
    "\n",
    "class Environment(EnvironmentBase):\n",
    "    def __init__(self, obstacle = [] ,id = 0, action_count=9, actionPrice = -1, goalReward = 100\n",
    "                 , punish=-10, j_limit = 10, i_limit = 10, p = 0.8, container=None):\n",
    "        \"\"\"\n",
    "        initialize your variables\n",
    "        \"\"\"\n",
    "        \n",
    "        self.obstacle = obstacle\n",
    "        \n",
    "        self.x_min = 1\n",
    "        self.x_max = i_limit\n",
    "        \n",
    "        self.y_min = 1\n",
    "        self.y_max = j_limit\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "        self.action_count = action_count\n",
    "        self.actionPrice = actionPrice\n",
    "        self.goalReward = goalReward\n",
    "        self.punish = punish\n",
    "        self.p = p\n",
    "        self.id = id\n",
    "        \n",
    "    def isStatePossible(self, state):\n",
    "        \"\"\"if given state is possible (not out of the grid and not obstacle) return ture\"\"\"\n",
    "        if self.x_min <= state[0] <= self.x_max and self.y_min <= state[1] <= self.y_max:\n",
    "            if state in self.obstacle:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def isAccessible(self, state, state_p):\n",
    "        \"\"\"if given state is Accesible (we can reach state_p by doing an action from state) return true\"\"\"\n",
    "        return abs(state[0]-state_p[0]) <= 1 and abs(state[1] - state_p[1]) <= 1 and self.isStatePossible(state_p)\n",
    "            \n",
    "    def getTransitionStatesAndProbs(self, state, action, state_p):\n",
    "        \"\"\"return probability of transition or T(sp,a,s)\"\"\"\n",
    "        \n",
    "        actions = self.available_actions_state(state)\n",
    "        \n",
    "        if self.calculate_next_state(state,action)==state_p:\n",
    "            if self.isAccessible(state, state_p):\n",
    "                return self.p + (1-self.p) / len(actions)\n",
    "            else:\n",
    "                return self.p\n",
    "        else:\n",
    "            if self.isAccessible(state, state_p):\n",
    "                return (1-self.p) / len(actions)\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "    \n",
    "    def getReward(self, state, action, state_p):\n",
    "        \"\"\"return reward of transition\"\"\"\n",
    "        if self.terminated_state(state_p):\n",
    "            return self.actionPrice + self.goalReward\n",
    "        \n",
    "        if self.isStatePossible(state_p):\n",
    "            return self.actionPrice\n",
    "        else:\n",
    "            return self.actionPrice + self.punish\n",
    "        \n",
    "    def sample_all_rewards(self):\n",
    "        return \n",
    "    \n",
    "    def calculate_reward(self, action):\n",
    "        return \n",
    "\n",
    "    def terminated(self):\n",
    "        return self.terminated_state(self.current_state)\n",
    "    \n",
    "    def terminated_state(self, state):\n",
    "        return state[0] == 1 and state[1] == 1\n",
    "        \n",
    "    def observe(self):\n",
    "        return \n",
    "\n",
    "    def available_actions(self):\n",
    "        return self.available_actions_state(self.current_state)\n",
    "    \n",
    "    def available_actions_state(self, state):\n",
    "        output_actions = []\n",
    "        for action in range(self.action_count):\n",
    "            next_state = self.calculate_next_state(state, action)\n",
    "            \n",
    "            if self.isAccessible(state, next_state):\n",
    "                output_actions.append(action)\n",
    "        \n",
    "        return output_actions\n",
    "        \n",
    "    \n",
    "    def calculate_next_state(self, state, action):\n",
    "        return (state[0] + (action%3 -1), state[1] + (int(action/3)-1) )\n",
    "        \n",
    "    def next_state(self, action):\n",
    "        actions = self.available_actions()\n",
    "        \n",
    "        if action not in actions:\n",
    "            actions.append(action)\n",
    "                \n",
    "        probabilities = []\n",
    "        \n",
    "        print(actions)\n",
    "        \n",
    "        for action2 in actions:\n",
    "            state2 = self.calculate_next_state(self.current_state, action2)\n",
    "            probabilities.append(self.getTransitionStatesAndProbs(self.current_state, action, state2))\n",
    "        \n",
    "        print(probabilities)\n",
    "        \n",
    "        final_action = random.choices(population=actions, weights=probabilities, k=1)[0]\n",
    "        \n",
    "        real_next_state = self.calculate_next_state(self.current_state, final_action)\n",
    "        \n",
    "        if not self.isStatePossible(real_next_state):\n",
    "            real_next_state = self.current_state\n",
    "        \n",
    "        self.last_action = action\n",
    "        \n",
    "        self.sliped = not (final_action==action)\n",
    "        \n",
    "        self.current_state = real_next_state\n",
    "        \n",
    "        return\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = (15,15)\n",
    "        self.current_state = (15,11)\n",
    "        \n",
    "        self.last_action = None\n",
    "        self.sliped = None\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"{self.current_state} \\t {self.last_action} \\t {self.sliped}\")\n",
    "        return \n",
    "\n",
    "    def close(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_states =  [(7, 1), (8, 1), (7, 2), (8, 2), (7, 3), (8, 3), (7, 4), (8, 4)\n",
    "                ,(13, 8), (14, 8), (15, 8), (13, 9), (14, 9), (15, 9)\n",
    "                ,(6, 12), (7, 12), (6, 13), (7, 13), (6, 14), (7, 14), (6, 15), (7, 15)]\n",
    "\n",
    "environment = Environment(obstacle = grid_states ,id = 0, action_count=9, actionPrice = -1, goalReward = 100\n",
    "                 , punish=-10, j_limit = 15, i_limit = 15, p = 0.8, container=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 11) \t None \t None\n"
     ]
    }
   ],
   "source": [
    "environment.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 4, 6, 7]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.available_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 11)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.getReward((15,10), 1, (15, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 6, 7, 1]\n",
      "[0.04999999999999999, 0.04999999999999999, 0.04999999999999999, 0.04999999999999999, 0.8]\n"
     ]
    }
   ],
   "source": [
    "environment.next_state(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 10) \t 1 \t False\n"
     ]
    }
   ],
   "source": [
    "environment.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "898Jlhsycyes"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Agent(AgentBase):\n",
    "    def __init__(self, id, environment, discount, theta):\n",
    "        #initialize a random policy and V(s) = 0 for each state\n",
    "        self.environment = environment\n",
    "        self.mapp = {}\n",
    "        #mapp states to its ids\n",
    "        self.V = {}\n",
    "        #init V\n",
    "        self.policy = {}\n",
    "        #init policy\n",
    "        super(Agent, self).__init__(id, environment)\n",
    "        self.discount = discount\n",
    "        self.theta = theta\n",
    "        \n",
    "    def policy_evaluation(self):\n",
    "        pass\n",
    "    \n",
    "    def policy_improvement(self):\n",
    "        pass\n",
    "    \n",
    "    def value_iteration(self):\n",
    "        pass\n",
    "    \n",
    "    def take_action(self) -> (object, float, bool, object):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "env.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "83fad98a7911d3a2a55c2e5234aea09e74d0252d0d10d90172c6e09f21426bdf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
